{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Environmental Science Analysis","text":"<p>Fall 2024 | 16:375:501 Environmental Science Analysis Rutgers University, Department of Environmental Sciences</p>"},{"location":"#schedule","title":"Schedule","text":"Week topic Week 1 Course introduction Week 2 Introduction to environmental datasets, Install Python (Windows, Mac, Amarel) Week 3 Core Python Language Week 4 Scientific Computing with Python Week 5 Working with tabular data in Python Week 6 Correlation and regressions Week 7 Time series analysis Week 8 Environmental data visualization Week 9 Spatial data analysis Week 10 Working with multi-dimensional data Week 11 Making maps Week 12 Reproducible research Week 13 Environmental science packages in Python Week 14 Final Project"},{"location":"Schedule/","title":"Schedule","text":""},{"location":"Schedule/#schedule","title":"Schedule","text":"Week topic Week 1 Course introduction Week 2 Introduction to environmental datasets, Install Python (Windows, Mac, Amarel) Week 3 Core Python Language Week 4 Scientific Computing with Python Week 5 Working with tabular data in Python Week 6 Correlation and regressions Week 7 Time series analysis Week 8 Environmental data visualization Week 9 Spatial data analysis Week 10 Working with multi-dimensional data Week 11 Making maps Week 12 Reproducible research Week 13 Environmental science packages in Python Week 14 Final Project"},{"location":"Syllabus/","title":"Syllabus","text":""},{"location":"Syllabus/#part-1-course-information","title":"Part 1: Course Information","text":"<p>Class Time and Location: Online Asynchronous </p>"},{"location":"Syllabus/#instructor","title":"Instructor:","text":"<p>Xiaomeng Jin Department of Environmental Sciences Office: ENR 230 Email: xiaomeng.jin@rutgers.edu Office Hour: TBD</p>"},{"location":"Syllabus/#part-2-overview","title":"Part 2: Overview","text":"<p>This course will introduce data analysis techniques for applications in environmental sciences. The course will teach students scientific programming in Python, statistical analysis, visualization, spatial analysis techniques that are commonly used to process and interpret environmental datasets. The course is designed to be accessible for graduate and upper-level undergraduate students in environmental sciences or other related disciplines.  </p>"},{"location":"Syllabus/#part-3-course-structure","title":"Part 3: Course Structure","text":"<p>Format: This is an online asynchronous course, meaning that we do not \u2018meet\u2019, not even via the web. Therefore, you decide when to do the work. To prevent you from procrastinating too much, you will have an assignment due each week for the first 12 weeks. Your assignment each week is to follow the instructions to complete a Jupyter notebook. By the end of the semester, you should have a notebook collection that you can use as coding recipe for your final project and your future research/work.   Textbook: There is no required textbook. All materials will come from free online resources and the course website itself.  Computers: Students will have the option to use their laptop, Amarel (the university\u2019s high performance computing cluster), or Google\u2019s Colaboratory (https://colab.research.google.com) to work on their assignments and final project.  </p>"},{"location":"Syllabus/#part-4-grading-policy","title":"Part 4: Grading Policy","text":""},{"location":"Syllabus/#weekly-assignments-70","title":"Weekly Assignments (70%)","text":"<p>Type 1: Python Programming \u2022   Total: 100 \u2022   All questions complete: 50 \u2022   All questions correct: 30  \u2022   Clean, elegant, efficient code: rate between 0 and 10  \u2022   Clear comments and explanations: rate between 0 and 10 </p> <p>Type 2: Weekly Quiz  \u2022   Multiple choice or short answer questions</p> <p>Lowest grade on an assignment will be dropped. </p>"},{"location":"Syllabus/#final-project-30","title":"Final Project (30%)","text":"<p>The goal of the final project is to assess your ability to combine and apply the skills you have learned in class in the context of a real-world research problem. Our class has mostly focused on tools for environmental data analysis, so this must be the focus of your final project. Specifically, we seek to assess your ability to do the following tasks:  \u2022   Discover and download real datasets in standard formats (e.g. CSV, netCDF)  \u2022   Load the data into pandas or xarray, performing any necessary data cleanup (dealing with missing values, proper time encoding, etc.) along the way.  \u2022   Perform realistic scientific calculation involving, for example tasks such as data grouping, aggregating, correlation analysis, trend analysis.  \u2022   Visualize your results in well-formatted plots.   \u2022   Clearly document your analysis to make it reproducible.  \u2022   Publish your final project as a GitHub repository. </p>"},{"location":"Syllabus/#grading","title":"Grading","text":"<p>\u2022   Total: 100  \u2022   Data: 30  \u2022   Statistical analysis: 30  \u2022   Visualization: 20  \u2022   Clean, efficient, reproducible code: 20 </p>"},{"location":"Week_2_install_python_amarel/","title":"Install Mamba and Python: Amarel User","text":"<ul> <li> <p>Request Amarel Account from Rutgers Office of Advanced Computing</p> </li> <li> <p>Connect to Amarel Open OnDemand </p> </li> <li>Click Clusters </li> <li>Choose Amarel Cluster Shell Access </li> <li>Enter your password </li> <li> <p>In the terminal, do the following commands (one line each time). If you're using Windows, type 'Ctrl+c' to copy and 'Ctrl+Shift+v' to paste command. </p> </li> <li> <p>Install Miniforge using: </p> </li> </ul> <pre><code>wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <pre><code>    Agree license term: yes \n    Confirm install location: ENTER\n    Do you wish to update your shell profile to automatically initialize conda: yes\n</code></pre> <ul> <li>Close terminal window, and reopen a terminal window. Test if Mamba and Conda have been successfully installed</li> </ul> <pre><code>mamba\n</code></pre> <p>You should see</p> <pre><code>conda is a tool for managing and deploying applications, environments and packages. \n....\n</code></pre> <ul> <li>Create a new environment <code>esa_env</code>: </li> </ul> <pre><code>mamba create -n esa_env python=3.9 jupyter jupyterlab notebook numpy scipy ipython pandas matplotlib cartopy geopandas xarray dask netCDF4 seaborn statsmodels pooch\n</code></pre> <ul> <li> <p>Next, go back to Amarel Open OnDemand. This time, we will launch a personal jupyter. Click on 'Interactive Apps', choose 'Personal Jupyter'. </p> </li> <li> <p>Settings for Personal Jupyter: </p> </li> </ul> <pre><code>    Number of hours: 10 \n    Number of cores: 1 \n    Gigabytes of memory: 10 \n    Partition: main\n    Leave Reservation and slurm feature blank \n    conda path: /home/YOURNETID/miniforge3\n    conda environment: esa_env\n</code></pre> <ul> <li> <p>You should see Jupyter lab page automatically opened in your web browser. </p> </li> <li> <p>In the Jupyter Lab, go to File -&gt; Open from Path. Enter the path of your home folder: /home/YOURNETID/</p> </li> <li> <p>Once you're at your home folder, open a new Jupyter notebook by clicking the + sign. Choose Notebook. </p> </li> <li> <p>Rename the Notebook to be 'Lecture_2_Install_Python.ipynb'</p> </li> <li> <p>Enter the following code in the Notebook to test if you successfully installed all necessary packages:</p> </li> </ul> <pre><code>import xarray as xr\nimport pandas as pd\nimport geopandas as gpd\n</code></pre> <p>If no errors, enter the following to test the plotting functions:</p> <pre><code>ds = xr.tutorial.load_dataset(\"air_temperature\")\nds.air[0,:,:].plot()\n</code></pre> <p>-Download and submit this Notebook as your assignment. </p>"},{"location":"Week_2_install_python_mac/","title":"Install Mamba and Python: Mac User","text":"<ul> <li> <p>Open Terminal (Launchpad -&gt; Other -&gt; Terminal)</p> </li> <li> <p>Install Miniforge using: </p> </li> </ul> <pre><code>curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh\n</code></pre> <ul> <li>Install Mamba on your laptop:</li> </ul> <pre><code>    Agree license term: yes \n    Confirm install location: ENTER\n    Do you wish to update your shell profile to automatically initialize conda: yes\n</code></pre> <ul> <li>Close terminal window, and reopen a terminal window. Test if Mamba and Conda have been successfully installed</li> </ul> <pre><code>mamba\n</code></pre> <p>You should see</p> <pre><code>conda is a tool for managing and deploying applications, environments and packages. \n....\n</code></pre> <ul> <li>In the terminal, create a new environment <code>esa_env</code>: </li> </ul> <pre><code>mamba create -n esa_env python=3.9 jupyter jupyterlab notebook numpy scipy ipython pandas matplotlib cartopy geopandas xarray dask netCDF4 seaborn statsmodels pooch\n</code></pre> <ul> <li>After it's finished, activate the environment: </li> </ul> <pre><code>mamba activate esa_env\n</code></pre> <ul> <li>Invoke Jupyter Lab: </li> </ul> <pre><code>jupyter lab\n</code></pre> <ul> <li> <p>You should see Jupyter lab page automatically opened in your web browser. If not, copy and paste one of the URLs listed to your web browser.</p> </li> <li> <p>In the Jupyter Lab, go to File -&gt; Open from Path. Enter the path of your course folder. </p> </li> <li> <p>If you don't know the folder path, find the folder in Finder, right click on the folder, press Option, and click Copy \"environmental_science_analysis\" as Pathname. </p> </li> <li> <p>Once you're at your course folder, open a new Jupyter notebook by clicking the + sign. Choose Notebook. </p> </li> <li> <p>Rename the Notebook to be 'Lecture_2_Install_Python.ipynb'</p> </li> <li> <p>Enter the following code in the Notebook to test if you successfully installed all necessary packages:</p> </li> </ul> <pre><code>import xarray as xr\nimport pandas as pd\nimport geopandas as gpd\n</code></pre> <p>If no errors, enter the following to test the plotting functions:</p> <pre><code>ds = xr.tutorial.load_dataset(\"air_temperature\")\nds.air[0,:,:].plot()\n</code></pre> <p>-Submit this Jupyter Notebook as your assignment. </p>"},{"location":"Week_2_install_python_windows/","title":"Install Mamba and Python: Windows User","text":"<ul> <li> <p>Go to Miniforge Website.</p> </li> <li> <p>Open Minoforge page, download and execute the Windows installer.</p> </li> <li> <p>When asked about the Installation Type: Choose 'Just Me'</p> </li> <li> <p>Follow the default settings for the Advanced Installation Option. </p> </li> <li> <p>After installing Miniforge, open the Miniforge Prompt </p> </li> <li> <p>Test if Mamba and Conda have been successfully installed by entering the following in Miniforge Prompt. </p> </li> </ul> <pre><code>mamba\n</code></pre> <p>You should see</p> <pre><code>conda is a tool for managing and deploying applications, environments and packages. \n....\n</code></pre> <ul> <li>In the terminal, create a new environment <code>esa_env</code>: </li> </ul> <pre><code>mamba create -n esa_env python=3.9 jupyter jupyterlab notebook numpy scipy ipython pandas matplotlib cartopy geopandas xarray dask netCDF4 seaborn statsmodels pooch\n</code></pre> <ul> <li>After it's finished, activate the environment: </li> </ul> <pre><code>mamba activate esa_env\n</code></pre> <ul> <li>Invoke Jupyter Lab and specify the directory (e.g., if your course folder is located at E disk, specify it as E:/): </li> </ul> <pre><code>jupyter lab --notebook-dir=E:/ \n</code></pre> <ul> <li> <p>You should see Jupyter lab page automatically opened in your web browser. If not, copy and paste one of the URLs listed to your web browser.</p> </li> <li> <p>In the Jupyter Lab, you should see the directory of the disk on the left. Browse through the content and enter your course folder by clicking on it. </p> </li> <li> <p>Once you're at your course folder, open a new Jupyter notebook by clicking the + sign. Choose Notebook. </p> </li> <li> <p>Rename the Notebook to be 'Lecture_2_Install_Python.ipynb'</p> </li> <li> <p>Enter the following code in the Notebook to test if you successfully installed all necessary packages:</p> </li> </ul> <pre><code>import xarray as xr\nimport pandas as pd\nimport geopandas as gpd\n</code></pre> <p>If no errors, enter the following to test the plotting functions:</p> <pre><code>ds = xr.tutorial.load_dataset(\"air_temperature\")\nds.air[0,:,:].plot()\n</code></pre> <p>-Submit this Jupyter Notebook as your assignment. </p>"},{"location":"Week_3_Core_Python/","title":"Week 3 Core Python Language","text":"In\u00a0[1]: Copied! <pre># comments are anything that comes after the \"#\" symbol\na = 1       # assign 1 to variable a\nb = \"hello\" # assign \"hello\" to variable b\n</pre> # comments are anything that comes after the \"#\" symbol a = 1       # assign 1 to variable a b = \"hello\" # assign \"hello\" to variable b <p>HintsThe following identifiers are used as reserved words, or keywords of the language, and cannot be used as ordinary identifiers. They must be spelled exactly as written here:</p> <pre><code>False      class      finally    is         return\nNone       continue   for        lambda     try\nTrue       def        from       nonlocal   while\nand        del        global     not        with\nas         elif       if         or         yield\nassert     else       import     pass\nbreak      except     in         raise</code></pre> <p>Additionally, the following a built in functions which are always available in your namespace once you open a python interpreter</p> <pre><code>abs() dict() help() min() setattr() all() dir() hex() next() slice() any()\ndivmod() id() object() sorted() ascii() enumerate() input() oct() staticmethod()\nbin() eval() int() open() str() bool() exec() isinstance() ord() sum() bytearray()\nfilter() issubclass() pow() super() bytes() float() iter() print() tuple()\ncallable() format() len() property() type() chr() frozenset() list() range()\nvars() classmethod() getattr() locals() repr() zip() compile() globals() map()\nreversed() __import__() complex() hasattr() max() round() delattr() hash()\nmemoryview() set()</code></pre> In\u00a0[2]: Copied! <pre># how to we see our variables?\nprint(a)\nprint(b)\nprint(a,b)\n</pre> # how to we see our variables? print(a) print(b) print(a,b) <pre>1\nhello\n1 hello\n</pre> <p>All variables are objects. Every object has a type (class). To find out what type your variables are</p> In\u00a0[3]: Copied! <pre># as a shortcut, iPython notebooks will automatically print whatever is on the last line\ntype(b)\n</pre> # as a shortcut, iPython notebooks will automatically print whatever is on the last line type(b) Out[3]: <pre>str</pre> In\u00a0[4]: Copied! <pre>type(a) is int\n</pre> type(a) is int Out[4]: <pre>True</pre> <p>Hints Different objects attributes and methods, which can be accessed via the syntax <code>variable.method</code></p> <p>IPython will autocomplete if you press <code>&lt;tab&gt;</code> to show you the methods available.</p> In\u00a0[5]: Copied! <pre># this returns the method itself\nb.capitalize\n</pre> # this returns the method itself b.capitalize Out[5]: <pre>&lt;function str.capitalize()&gt;</pre> In\u00a0[6]: Copied! <pre># this calls the method\nb.capitalize()\n# there are lots of other methods\n</pre> # this calls the method b.capitalize() # there are lots of other methods Out[6]: <pre>'Hello'</pre> In\u00a0[7]: Copied! <pre># binary operations act differently on different types of objects\nc = 'World'\nprint(b + c)\nprint(a + 2)\nprint(a + b)\n</pre> # binary operations act differently on different types of objects c = 'World' print(b + c) print(a + 2) print(a + b) <pre>helloWorld\n3\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 5\n      3 print(b + c)\n      4 print(a + 2)\n----&gt; 5 print(a + b)\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[8]: Copied! <pre># addition / subtraction\n1+1-5\n</pre> # addition / subtraction 1+1-5 Out[8]: <pre>-3</pre> In\u00a0[9]: Copied! <pre># multiplication\n5 * 10\n</pre> # multiplication 5 * 10 Out[9]: <pre>50</pre> In\u00a0[10]: Copied! <pre># division\n1/2\n</pre> # division 1/2 Out[10]: <pre>0.5</pre> In\u00a0[11]: Copied! <pre># that was automatically converted to a float\ntype(1/2)\n</pre> # that was automatically converted to a float type(1/2) Out[11]: <pre>float</pre> In\u00a0[12]: Copied! <pre># exponentiation\n2**4\n</pre> # exponentiation 2**4 Out[12]: <pre>16</pre> In\u00a0[13]: Copied! <pre># rounding\nround(9/10)\n</pre> # rounding round(9/10) Out[13]: <pre>1</pre> In\u00a0[14]: Copied! <pre># built in complex number support\n(1+2j) / (3-4j)\n</pre> # built in complex number support (1+2j) / (3-4j) Out[14]: <pre>(-0.2+0.4j)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[15]: Copied! <pre>True and True\n</pre> True and True Out[15]: <pre>True</pre> In\u00a0[16]: Copied! <pre>True and False\n</pre> True and False Out[16]: <pre>False</pre> In\u00a0[17]: Copied! <pre># logic\n\n(not True) or (not False)\n</pre> # logic  (not True) or (not False) Out[17]: <pre>True</pre> In\u00a0[18]: Copied! <pre>True or True\n</pre> True or True Out[18]: <pre>True</pre> In\u00a0[19]: Copied! <pre>x = 4\nx &lt; 5 and  x &lt; 10\n</pre> x = 4 x &lt; 5 and  x &lt; 10 Out[19]: <pre>True</pre> In\u00a0[20]: Copied! <pre>x = 4\nx &lt; 5 or x &lt; 4\n</pre> x = 4 x &lt; 5 or x &lt; 4 Out[20]: <pre>True</pre> In\u00a0[21]: Copied! <pre>x = 4\nx &lt; 5 and x &lt; 4\n</pre> x = 4 x &lt; 5 and x &lt; 4 Out[21]: <pre>False</pre> In\u00a0[22]: Copied! <pre>not(x &lt; 5 and x &lt; 10)\n</pre> not(x &lt; 5 and x &lt; 10) Out[22]: <pre>False</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[50]: Copied! <pre>x = 100\nif x &gt; 0:\n    print('Positive Number')\nelif x &lt; 0:\n    print('Negative Number')\nelse:\n    print ('Zero!')\n</pre> x = 100 if x &gt; 0:     print('Positive Number') elif x &lt; 0:     print('Negative Number') else:     print ('Zero!') <pre>Positive Number\n</pre> In\u00a0[51]: Copied! <pre># Blocks are closed by indentation level\nif x &gt; 0:\n    print('Positive Number')\n    if x &gt;= 100:\n        print('Huge number!')\n</pre> # Blocks are closed by indentation level if x &gt; 0:     print('Positive Number')     if x &gt;= 100:         print('Huge number!') <pre>Positive Number\nHuge number!\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[52]: Copied! <pre># make a loop \ncount = 0\nwhile count &lt; 10:\n    # bad way\n    # count = count + 1\n    # better way\n    count += 1\nprint(count)\n</pre> # make a loop  count = 0 while count &lt; 10:     # bad way     # count = count + 1     # better way     count += 1 print(count) <pre>10\n</pre> <p>Hint: Be careful with the conditions. Avoid defining an infinite loop.</p> In\u00a0[53]: Copied! <pre># use range\nfor i in range(5):\n    print(i)\n</pre> # use range for i in range(5):     print(i) <pre>0\n1\n2\n3\n4\n</pre> In\u00a0[54]: Copied! <pre># Reverse the order\nfor i in range(5,0,-1):\n    print(i)\n</pre> # Reverse the order for i in range(5,0,-1):     print(i) <pre>5\n4\n3\n2\n1\n</pre> <p>Important point: in python, we always count from 0!</p> In\u00a0[55]: Copied! <pre># what is range?\ntype(range)\n</pre> # what is range? type(range) Out[55]: <pre>type</pre> In\u00a0[56]: Copied! <pre>range?\n</pre> range? <pre>Init signature: range(self, /, *args, **kwargs)\nDocstring:     \nrange(stop) -&gt; range object\nrange(start, stop[, step]) -&gt; range object\n\nReturn an object that produces a sequence of integers from start (inclusive)\nto stop (exclusive) by step.  range(i, j) produces i, i+1, i+2, ..., j-1.\nstart defaults to 0, and stop is omitted!  range(4) produces 0, 1, 2, 3.\nThese are exactly the valid indices for a list of 4 elements.\nWhen step is given, it specifies the increment (or decrement).\nType:           type\nSubclasses:     </pre> In\u00a0[57]: Copied! <pre># iterate over a list we make up\nfor pet in ['dog', 'cat', 'fish']:\n    print(pet, len(pet))\n</pre> # iterate over a list we make up for pet in ['dog', 'cat', 'fish']:     print(pet, len(pet)) <pre>dog 3\ncat 3\nfish 4\n</pre> <p>What is the thing in brackets? A list! Lists are one of the core python data structures.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[58]: Copied! <pre>l = ['dog', 'cat', 'fish']\ntype(l)\n</pre> l = ['dog', 'cat', 'fish'] type(l) Out[58]: <pre>list</pre> In\u00a0[37]: Copied! <pre># list have lots of methods\nl.sort()\nl\n</pre> # list have lots of methods l.sort() l Out[37]: <pre>['cat', 'dog', 'fish']</pre> In\u00a0[38]: Copied! <pre># we can convert a range to a list\nr = list(range(5))\nr\n</pre> # we can convert a range to a list r = list(range(5)) r Out[38]: <pre>[0, 1, 2, 3, 4]</pre> In\u00a0[39]: Copied! <pre>while r:\n    p = r.pop()\n    print('p:', p)\n    print('r:', r)\n</pre> while r:     p = r.pop()     print('p:', p)     print('r:', r) <pre>p: 4\nr: [0, 1, 2, 3]\np: 3\nr: [0, 1, 2]\np: 2\nr: [0, 1]\np: 1\nr: [0]\np: 0\nr: []\n</pre> <p>There are many different ways to interact with lists. Exploring them is part of the fun of python.</p> <p>list.append(x) Add an item to the end of the list. Equivalent to a[len(a):] = [x].</p> <p>list.extend(L) Extend the list by appending all the items in the given list. Equivalent to a[len(a):] = L.</p> <p>list.insert(i, x) Insert an item at a given position. The first argument is the index of the element before which to insert, so a.insert(0, x) inserts at the front of the list, and a.insert(len(a), x) is equivalent to a.append(x).</p> <p>list.remove(x) Remove the first item from the list whose value is x. It is an error if there is no such item.</p> <p>list.pop([i]) Remove the item at the given position in the list, and return it. If no index is specified, a.pop() removes and returns the last item in the list. (The square brackets around the i in the method signature denote that the parameter is optional, not that you should type square brackets at that position. You will see this notation frequently in the Python Library Reference.)</p> <p>list.clear() Remove all items from the list. Equivalent to del a[:].</p> <p>list.index(x) Return the index in the list of the first item whose value is x. It is an error if there is no such item.</p> <p>list.count(x) Return the number of times x appears in the list.</p> <p>list.sort() Sort the items of the list in place.</p> <p>list.reverse() Reverse the elements of the list in place.</p> <p>list.copy() Return a shallow copy of the list. Equivalent to a[:].</p> <p>Don't assume you know how list operations work!</p> In\u00a0[40]: Copied! <pre># \"add\" two lists\nx = list(range(5))\ny = list(range(10,15))\nz = x + y\nz\n</pre> # \"add\" two lists x = list(range(5)) y = list(range(10,15)) z = x + y z Out[40]: <pre>[0, 1, 2, 3, 4, 10, 11, 12, 13, 14]</pre> In\u00a0[41]: Copied! <pre># access items from a list\nprint('first', z[0])\nprint('last', z[-1])\nprint('first 3', z[:3])\nprint('last 3', z[-3:])\nprint('middle, skipping every other item', z[5:10:2])\n</pre> # access items from a list print('first', z[0]) print('last', z[-1]) print('first 3', z[:3]) print('last 3', z[-3:]) print('middle, skipping every other item', z[5:10:2]) <pre>first 0\nlast 14\nfirst 3 [0, 1, 2]\nlast 3 [12, 13, 14]\nmiddle, skipping every other item [10, 12, 14]\n</pre> <p>MEMORIZE THIS SYNTAX! It is central to so much of python and often proves confusing for users coming from other languages.</p> <p>In terms of set notation, python indexing is left inclusive, right exclusive. If you remember this, you will never go wrong.</p> In\u00a0[42]: Copied! <pre># that means we get an error from the following\nN = len(z)\nz[N]\n</pre> # that means we get an error from the following N = len(z) z[N] <pre>\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[42], line 3\n      1 # that means we get an error from the following\n      2 N = len(z)\n----&gt; 3 z[N]\n\nIndexError: list index out of range</pre> In\u00a0[43]: Copied! <pre># this index notation also applies to strings\nname = 'Xiaomeng Jin'\nprint(name[:4])\n</pre> # this index notation also applies to strings name = 'Xiaomeng Jin' print(name[:4]) <pre>Xiao\n</pre> In\u00a0[44]: Copied! <pre>print(name[:-4])\n</pre> print(name[:-4]) <pre>Xiaomeng\n</pre> In\u00a0[45]: Copied! <pre>print(name[-3:])\n</pre> print(name[-3:]) <pre>Jin\n</pre> In\u00a0[46]: Copied! <pre># you can also test for the presence of items in a list\n5 in z\n</pre> # you can also test for the presence of items in a list 5 in z Out[46]: <pre>False</pre> <p>Lists are not meant for math! They don't have a datatype.</p> In\u00a0[47]: Copied! <pre>z[4] = 'fish'\nz\n</pre> z[4] = 'fish' z Out[47]: <pre>[0, 1, 2, 3, 'fish', 10, 11, 12, 13, 14]</pre> <p>Python is full of tricks for iterating and working with lists</p> In\u00a0[48]: Copied! <pre># a cool python trick: list comprehension\nsquares = [n**2 for n in range(5)]\nsquares\n</pre> # a cool python trick: list comprehension squares = [n**2 for n in range(5)] squares Out[48]: <pre>[0, 1, 4, 9, 16]</pre> In\u00a0[49]: Copied! <pre># iterate over two lists together uzing zip\nfor item1, item2 in zip(x,y):\n    print('first:', item1, 'second:', item2)\n</pre> # iterate over two lists together uzing zip for item1, item2 in zip(x,y):     print('first:', item1, 'second:', item2) <pre>first: 0 second: 10\nfirst: 1 second: 11\nfirst: 2 second: 12\nfirst: 3 second: 13\nfirst: 4 second: 14\n</pre> <p>Define a list from a range of values, check if an element exists in the list.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>In this problem, we will explore the basic data structures and flow controls of python by manually parsing a CSV file.</p> <p>Note that this is a futile exercise. In the \"real world\" you should never manually parse a CSV file. There are utilities out there that will do it for you much more quickly and efficiently. However, it is a useful exercise for learning python.</p> <p>Before starting the python part, use the JupyterLab file browser to browse to this file. Click to open it. What do you see?</p> <p>Now we will begin the process of reading the file with python</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>It should be a familiar type we learned about in class.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Week_3_Core_Python/#week-3-core-python-language","title":"Week 3 Core Python Language\u00b6","text":"<p>Mostly copied from the official python tutorial</p>"},{"location":"Week_3_Core_Python/#1-invoking-python","title":"1. Invoking Python\u00b6","text":"<p>There are three main ways to use python.</p> <ol> <li>By running a python file, e.g. <code>python myscript.py</code></li> <li>Through an interactive console (python interpreter or ipython shell)</li> <li>In an interactive Jupyter Lab</li> </ol> <p>We will be using the Jupyter Lab.</p>"},{"location":"Week_3_Core_Python/#2-basic-variables-numbers-and-string","title":"2. Basic Variables: Numbers and String\u00b6","text":""},{"location":"Week_3_Core_Python/#exercise-2","title":"Exercise 2:\u00b6","text":"<p>Define a float variable d = 8. Print the type of the variable.</p>"},{"location":"Week_3_Core_Python/#3-math","title":"3. Math\u00b6","text":"<p>Basic arithmetic and boolean logic is part of the core python library.</p>"},{"location":"Week_3_Core_Python/#exercise-3","title":"Exercise 3:\u00b6","text":"<p>Let a = 2, b = 3.3, c = 2, calculate the following equation.</p> <p>$y = 6a^3 - \\frac{8b^2}{4c} + 11$</p>"},{"location":"Week_3_Core_Python/#4-logic","title":"4. Logic:\u00b6","text":"<p><code>and</code>: Returns True if both statements are true</p> <p><code>or</code>: Returns True if one of the statements is true</p> <p><code>not</code>: Reverse the result, returns False if the result is true</p>"},{"location":"Week_3_Core_Python/#exercise-4","title":"Exercise 4:\u00b6","text":"<p>For each of the following expressions, guess whether they evaluate to <code>True</code> or <code>False</code>. Then type them to check your answers.</p> <ol> <li>1&lt;=1</li> <li>1!=1</li> <li>123 == '123'</li> <li>1!=1 and 1&lt;=1</li> <li>1!=1 or 1&lt;=1</li> <li>'good' != 'bad'</li> </ol>"},{"location":"Week_3_Core_Python/#5-conditionals","title":"5. Conditionals\u00b6","text":"<p>Conditional statements are an essential part of programming in Python. They allow you to make decisions based on the values of variables or the result of comparisons.</p> <p>Hint: In Python, indentation is MANDATORY. Blocks are closed by indentation level.</p>"},{"location":"Week_3_Core_Python/#exercise-5","title":"Exercise 5:\u00b6","text":"<p>Given a number <code>x</code>, print if it is an odd or even number.</p>"},{"location":"Week_3_Core_Python/#6-loop","title":"6 Loop\u00b6","text":"<p>A loop is an instruction that repeats multiple times as long as some condition is met. Loops are useful for iterating over sequences (like lists, strings, or ranges) or performing a task multiple times.</p>"},{"location":"Week_3_Core_Python/#while-loop","title":"while loop\u00b6","text":"<p>A while loop repeatedly executes a block of code as long as a specified condition is True.</p>"},{"location":"Week_3_Core_Python/#for-loop","title":"for loop\u00b6","text":"<p>A for loop is used to iterate over a sequence (such as a list, tuple, string, or range) and execute a block of code for each item in that sequence.</p>"},{"location":"Week_3_Core_Python/#exercise","title":"Exercise:\u00b6","text":"<p>Create a loop from 100 to 1, if odd number, print 'odd' and the number, or print 'even' and the number.</p>"},{"location":"Week_3_Core_Python/#7-lists","title":"7. Lists\u00b6","text":"<p>In Python, a list is a built-in data type that allows you to store an ordered collection of items. These items can be of any data type, including integers, strings, floating-point numbers, or even other lists. Lists are mutable, meaning that you can change their content after they've been created (e.g., by adding, removing, or modifying elements).</p>"},{"location":"Week_3_Core_Python/#exercise-7","title":"Exercise 7:\u00b6","text":""},{"location":"Week_3_Core_Python/#8-assignment-python-lists-and-loops","title":"8. Assignment:  Python Lists and Loops\u00b6","text":""},{"location":"Week_3_Core_Python/#81-open-the-file-using-the-open-function","title":"8.1 Open the file using the <code>open</code> function\u00b6","text":"<p>Specifically, run the command</p> <pre><code>file = open('esa_roster_fall_2024.csv')</code></pre>"},{"location":"Week_3_Core_Python/#82-use-the-help-function-to-get-the-documentation-for-your-new-variable-file","title":"8.2 Use the <code>help</code> function to get the documentation for your new variable <code>file</code>\u00b6","text":"<p>This will produce a long list of methods you can use with <code>file</code>.</p>"},{"location":"Week_3_Core_Python/#83-read-the-lines-of-the-file-into-a-variable-called-lines","title":"8.3 Read the lines of the file into a variable called <code>lines</code>\u00b6","text":"<p>Hint: use the documentation above to find the method that sounds most likely to do what you want.</p>"},{"location":"Week_3_Core_Python/#84-display-lines-at-the-end-of-a-cell-in-order-to-see-its-contents","title":"8.4 Display <code>lines</code> at the end of a cell in order to see its contents\u00b6","text":""},{"location":"Week_3_Core_Python/#85-display-the-number-of-students-in-class","title":"8.5 Display the number of students in class\u00b6","text":""},{"location":"Week_3_Core_Python/#86-use-slicing-to-display-the-first-three-items-of-the-list-and-the-last-3","title":"8.6 Use slicing to display the first three items of the list. And the last 3\u00b6","text":""},{"location":"Week_3_Core_Python/#87-now-iterate-through-lines-and-print-the-item-if-it-contains-your-netid","title":"8.7 Now iterate through <code>lines</code> and <code>print</code> the item if it contains your NetID\u00b6","text":""},{"location":"Week_4_Numpy/","title":"Week 4: Scientific Computing in Python","text":"In\u00a0[1]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[4]: Copied! <pre># find out what version we have\nnp.__version__\n</pre> # find out what version we have np.__version__ Out[4]: <pre>'2.0.0'</pre> In\u00a0[\u00a0]: Copied! <pre># find out what is in our namespace\ndir()\n</pre> # find out what is in our namespace dir() In\u00a0[\u00a0]: Copied! <pre># find out what's in numpy\ndir(np)\n</pre> # find out what's in numpy dir(np) <p>The numpy documentation is crucial!</p> <p>http://docs.scipy.org/doc/numpy/reference/</p> In\u00a0[14]: Copied! <pre>from IPython.display import Image\nImage(url='http://docs.scipy.org/doc/numpy/_images/threefundamental.png')\n</pre> from IPython.display import Image Image(url='http://docs.scipy.org/doc/numpy/_images/threefundamental.png') Out[14]: In\u00a0[7]: Copied! <pre># create an array from a list\na = np.array([9,0,2,1,0])\n</pre> # create an array from a list a = np.array([9,0,2,1,0]) In\u00a0[8]: Copied! <pre># find out the datatype\na.dtype\n</pre> # find out the datatype a.dtype Out[8]: <pre>dtype('int64')</pre> In\u00a0[9]: Copied! <pre># find out the shape\na.shape\n</pre> # find out the shape a.shape Out[9]: <pre>(5,)</pre> In\u00a0[10]: Copied! <pre># what is the shape\ntype(a.shape)\n</pre> # what is the shape type(a.shape) Out[10]: <pre>tuple</pre> In\u00a0[11]: Copied! <pre># another array with a different datatype and shape\nb = np.array([[5,3,1,9],[9,2,3,0]], dtype=np.float64)\n</pre> # another array with a different datatype and shape b = np.array([[5,3,1,9],[9,2,3,0]], dtype=np.float64) In\u00a0[12]: Copied! <pre># array with 3 rows x 4 columns\na_2d = np.array([[3,2,0,1],[9,1,8,7],[4,0,1,6]]) \na_2d\n</pre> # array with 3 rows x 4 columns a_2d = np.array([[3,2,0,1],[9,1,8,7],[4,0,1,6]])  a_2d Out[12]: <pre>array([[3, 2, 0, 1],\n       [9, 1, 8, 7],\n       [4, 0, 1, 6]])</pre> In\u00a0[13]: Copied! <pre># check dtype and shape\nb.dtype, b.shape\n</pre> # check dtype and shape b.dtype, b.shape Out[13]: <pre>(dtype('float64'), (2, 4))</pre> <p>Important Concept: The fastest varying dimension is the last dimension! The outer level of the hierarchy is the first dimension. (This is called \"c-style\" indexing)</p> In\u00a0[73]: Copied! <pre># create some uniform arrays\nc = np.zeros((9,9))\nd = np.ones((3,6,3), dtype=np.complex128)\ne = np.full((3,3), np.pi)\ne = np.ones_like(c)\nf = np.zeros_like(d)\n# Random arrays\ng = np.random.rand(3,4)\n</pre> # create some uniform arrays c = np.zeros((9,9)) d = np.ones((3,6,3), dtype=np.complex128) e = np.full((3,3), np.pi) e = np.ones_like(c) f = np.zeros_like(d) # Random arrays g = np.random.rand(3,4) <p>The <code>np.arange()</code> function is used to generate an array with evenly spaced values within a given interval. <code>np.arange()</code> can be used with one, two, or three parameters to specify the start, stop, and step values. If only one value is passed to the function, it will be interpreted as the stop value:</p> In\u00a0[16]: Copied! <pre># create some ranges\nnp.arange(10)\n</pre> # create some ranges np.arange(10) Out[16]: <pre>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> In\u00a0[17]: Copied! <pre># arange is left inclusive, right exclusive\nnp.arange(2,4,0.25)\n</pre> # arange is left inclusive, right exclusive np.arange(2,4,0.25) Out[17]: <pre>array([2.  , 2.25, 2.5 , 2.75, 3.  , 3.25, 3.5 , 3.75])</pre> <p>Similarly, the <code>np.linspace()</code> function is used to construct an array with evenly spaced numbers over a given interval. However, instead of the step parameter, <code>np.linspace()</code> takes a num parameter to specify the number of samples within the given interval:</p> In\u00a0[18]: Copied! <pre># linearly spaced\nnp.linspace(2,4,20)\n</pre> # linearly spaced np.linspace(2,4,20) Out[18]: <pre>array([2.        , 2.10526316, 2.21052632, 2.31578947, 2.42105263,\n       2.52631579, 2.63157895, 2.73684211, 2.84210526, 2.94736842,\n       3.05263158, 3.15789474, 3.26315789, 3.36842105, 3.47368421,\n       3.57894737, 3.68421053, 3.78947368, 3.89473684, 4.        ])</pre> <p>Note that unlike <code>np.arange()</code>, <code>np.linspace()</code> includes the stop value by default (this can be changed by passing <code>endpoint=True</code>). Finally, it should be noted that while we could have used <code>np.arange()</code> to generate the same array in the above example, it is recommended to use <code>np.linspace()</code> when a non-integer step (e.g. 0.25) is desired.</p> In\u00a0[19]: Copied! <pre>np.linspace(2,4,20, endpoint = False)\n</pre> np.linspace(2,4,20, endpoint = False) Out[19]: <pre>array([2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3. , 3.1, 3.2,\n       3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[20]: Copied! <pre>x = np.linspace(-4, 4, 9)\nx\n</pre> x = np.linspace(-4, 4, 9) x Out[20]: <pre>array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])</pre> In\u00a0[21]: Copied! <pre>y = np.linspace(-5, 5, 11)\ny\n</pre> y = np.linspace(-5, 5, 11) y Out[21]: <pre>array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])</pre> In\u00a0[22]: Copied! <pre>x_2d, y_2d = np.meshgrid(x, y)\n</pre>   x_2d, y_2d = np.meshgrid(x, y) In\u00a0[23]: Copied! <pre>x_2d\n</pre> x_2d Out[23]: <pre>array([[-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.],\n       [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.]])</pre> In\u00a0[24]: Copied! <pre>y_2d\n</pre> y_2d Out[24]: <pre>array([[-5., -5., -5., -5., -5., -5., -5., -5., -5.],\n       [-4., -4., -4., -4., -4., -4., -4., -4., -4.],\n       [-3., -3., -3., -3., -3., -3., -3., -3., -3.],\n       [-2., -2., -2., -2., -2., -2., -2., -2., -2.],\n       [-1., -1., -1., -1., -1., -1., -1., -1., -1.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.],\n       [ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.],\n       [ 4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.],\n       [ 5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.]])</pre> In\u00a0[25]: Copied! <pre># get some individual elements of xx\nx_2d[0,0], x_2d[-1,-1], x_2d[3,-5]\n</pre> # get some individual elements of xx x_2d[0,0], x_2d[-1,-1], x_2d[3,-5] Out[25]: <pre>(np.float64(-4.0), np.float64(4.0), np.float64(0.0))</pre> In\u00a0[26]: Copied! <pre># get some whole rows and columns\nx_2d[0].shape, x_2d[:,-1].shape\n</pre> # get some whole rows and columns x_2d[0].shape, x_2d[:,-1].shape Out[26]: <pre>((9,), (11,))</pre> In\u00a0[27]: Copied! <pre># get some ranges\nx_2d[3:10,3:5].shape\n</pre> # get some ranges x_2d[3:10,3:5].shape Out[27]: <pre>(7, 2)</pre> <p>There are many advanced ways to index arrays. You can read about them in the manual. Here is one example.</p> In\u00a0[28]: Copied! <pre># use a boolean array as an index\nidx = x_2d&lt;0\nx_2d[idx].shape\n</pre> # use a boolean array as an index idx = x_2d&lt;0 x_2d[idx].shape Out[28]: <pre>(44,)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[32]: Copied! <pre># two dimensional grids\nx = np.linspace(-2*np.pi, 2*np.pi, 100)\ny = np.linspace(-np.pi, np.pi, 50)\nxx, yy = np.meshgrid(x, y)\nxx.shape, yy.shape\n</pre> # two dimensional grids x = np.linspace(-2*np.pi, 2*np.pi, 100) y = np.linspace(-np.pi, np.pi, 50) xx, yy = np.meshgrid(x, y) xx.shape, yy.shape Out[32]: <pre>((50, 100), (50, 100))</pre> In\u00a0[33]: Copied! <pre>f = np.sin(xx) * np.cos(0.5*yy)\n</pre> f = np.sin(xx) * np.cos(0.5*yy) In\u00a0[34]: Copied! <pre>from matplotlib import pyplot as plt\n</pre> from matplotlib import pyplot as plt In\u00a0[35]: Copied! <pre>plt.pcolormesh(f)\n</pre> plt.pcolormesh(f) Out[35]: <pre>&lt;matplotlib.collections.QuadMesh at 0x117fdee80&gt;</pre> In\u00a0[36]: Copied! <pre># transpose\nplt.pcolormesh(f.T)\n</pre> # transpose plt.pcolormesh(f.T) Out[36]: <pre>&lt;matplotlib.collections.QuadMesh at 0x12624e400&gt;</pre> In\u00a0[37]: Copied! <pre>f.shape\n</pre> f.shape Out[37]: <pre>(50, 100)</pre> In\u00a0[38]: Copied! <pre>np.tile(f,(6,1)).shape\n</pre> np.tile(f,(6,1)).shape Out[38]: <pre>(300, 100)</pre> In\u00a0[39]: Copied! <pre># tile an array\nplt.pcolormesh(np.tile(f,(6,1)))\n</pre> # tile an array plt.pcolormesh(np.tile(f,(6,1))) Out[39]: <pre>&lt;matplotlib.collections.QuadMesh at 0x1262bbeb0&gt;</pre> In\u00a0[44]: Copied! <pre>from IPython.display import Image\nImage(url='http://scipy-lectures.github.io/_images/numpy_broadcasting.png',\n     width=720)\n</pre> from IPython.display import Image Image(url='http://scipy-lectures.github.io/_images/numpy_broadcasting.png',      width=720) Out[44]: In\u00a0[45]: Copied! <pre># multiply f by x\nprint(f.shape, x.shape)\ng = f * x\nprint(g.shape)\n</pre> # multiply f by x print(f.shape, x.shape) g = f * x print(g.shape) <pre>(50, 100) (100,)\n(50, 100)\n</pre> In\u00a0[46]: Copied! <pre># multiply f by y\nprint(f.shape, y.shape)\nh = f * y\nprint(h.shape)\n</pre> # multiply f by y print(f.shape, y.shape) h = f * y print(h.shape) <pre>(50, 100) (50,)\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[46], line 3\n      1 # multiply f by y\n      2 print(f.shape, y.shape)\n----&gt; 3 h = f * y\n      4 print(h.shape)\n\nValueError: operands could not be broadcast together with shapes (50,100) (50,) </pre> In\u00a0[69]: Copied! <pre># use newaxis special syntax\ny_new = y[:,np.newaxis]\nh = f * y_new\nprint(h.shape)\n</pre> # use newaxis special syntax y_new = y[:,np.newaxis] h = f * y_new print(h.shape) <pre>(50, 100)\n</pre> In\u00a0[48]: Copied! <pre># sum\ng.sum()\n</pre> # sum g.sum() Out[48]: <pre>np.float64(-3083.038387807155)</pre> In\u00a0[49]: Copied! <pre># mean\ng.mean()\n</pre> # mean g.mean() Out[49]: <pre>np.float64(-0.616607677561431)</pre> In\u00a0[50]: Copied! <pre># std\ng.std()\n</pre> # std g.std() Out[50]: <pre>np.float64(1.6402280119141424)</pre> In\u00a0[51]: Copied! <pre># apply on just one axis\n\n# Mean of each row (calculated across columns)\ng_xmean = g.mean(axis=1)\n\n# Mean of each column (calculated across rows)\n\ng_ymean = g.mean(axis=0)\n</pre> # apply on just one axis  # Mean of each row (calculated across columns) g_xmean = g.mean(axis=1)  # Mean of each column (calculated across rows)  g_ymean = g.mean(axis=0) In\u00a0[52]: Copied! <pre>plt.plot(x, g_ymean)\n</pre> plt.plot(x, g_ymean) Out[52]: <pre>[&lt;matplotlib.lines.Line2D at 0x1265e17f0&gt;]</pre> In\u00a0[53]: Copied! <pre>plt.plot(g_xmean, y)\n</pre> plt.plot(g_xmean, y) Out[53]: <pre>[&lt;matplotlib.lines.Line2D at 0x1266d3eb0&gt;]</pre> <p>Most real-world datasets \u2013 environmental or otherwise \u2013 have data gaps. Data can be missing for any number of reasons, including observations not being recorded or data corruption. While a cell corresponding to a data gap may just be left blank in a spreadsheet, when imported into Python, there must be some way to handle \"blank\" or missing values.</p> <p>Missing data should not be replaced with zeros, as 0 can be a valid value for many datasets, (e.g. temperature, precipitation, etc.). Instead, the convention is to fill all missing data with the constant NaN. NaN stands for \"Not a Number\" and is implemented in NumPy as np.nan.</p> <p>NaNs are handled differently by different packages. In NumPy, all computations involving NaN values will return nan:</p> In\u00a0[70]: Copied! <pre>data = np.array([[2.,2.7,1.89],\n                 [1.1, 0.0, np.nan],\n                 [3.2, 0.74, 2.1]])\n</pre> data = np.array([[2.,2.7,1.89],                  [1.1, 0.0, np.nan],                  [3.2, 0.74, 2.1]]) In\u00a0[71]: Copied! <pre>np.mean(data)\n</pre> np.mean(data) Out[71]: <pre>np.float64(nan)</pre> In\u00a0[72]: Copied! <pre>np.nanmean(data)\n</pre> np.nanmean(data) Out[72]: <pre>np.float64(1.71625)</pre> <p>First import numpy and matplotlib</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Week_4_Numpy/#week-4-scientific-computing-in-python","title":"Week 4: Scientific Computing in Python\u00b6","text":"<p>This week, you will learn how to do scientific computing in Python. As we learned from first lecture, Numpy is a fundamental package for scientific computing.</p> <p>The goal of this assignment is to gain comfort creating, visualizating, and computing with numpy array. By the end of the assignment, you should feel comfortable:</p> <ul> <li>Creating new Numpy arrays using <code>linspace</code> and <code>arange</code></li> <li>Computing basic formulas with Numpy arrays</li> <li>Performing reductions (e.g. <code>mean</code>, <code>std</code> on numpy arrays)</li> <li>Making 1D line plots</li> </ul>"},{"location":"Week_4_Numpy/#1-importing-and-examining-a-new-package","title":"1. Importing and Examining a New Package\u00b6","text":"<p>This will be our first experience with importing a package which is not part of the Python standard library.</p>"},{"location":"Week_4_Numpy/#2-ndarrays","title":"2. NDArrays\u00b6","text":"<p>The core class is the numpy ndarray (n-dimensional array). The n-dimensional array object in NumPy is referred to as an ndarray, a multidimensional container of homogeneous items \u2013 i.e. all values in the array are the same type and size. These arrays can be one-dimensional (one row or column vector), two-dimensional (m rows x n columns), or three-dimensional (arrays within arrays).</p> <p>The main difference between a numpy array an a more general data container such as <code>list</code> are the following:</p> <ul> <li>Numpy arrays can have multiple dimensions (while lists, tuples, etc. only have 1)</li> <li>Numpy arrays hold values of the same datatype (e.g. <code>int</code>, <code>float</code>), while <code>lists</code> can contain anything.</li> <li>Numpy optimizes numerical operations on arrays. Numpy is fast!</li> </ul>"},{"location":"Week_4_Numpy/#21-create-array-from-a-list","title":"2.1 Create array from a list\u00b6","text":""},{"location":"Week_4_Numpy/#22-create-arrays-using-functions","title":"2.2 Create arrays using functions\u00b6","text":""},{"location":"Week_4_Numpy/#exercise-1-create-a-1d-array-ranging-from-0-to-100-including-100-with-an-interval-of-5","title":"Exercise 1: Create a 1D array ranging from 0 to 100 (including 100) with an interval of 5.\u00b6","text":""},{"location":"Week_4_Numpy/#23-create-two-dimensional-grids","title":"2.3 Create two-dimensional grids\u00b6","text":""},{"location":"Week_4_Numpy/#exercise-3-explain-what-the-meshgrid-function-does-what-is-the-difference-between-x_2d-and-y_2d","title":"Exercise 3: Explain what the meshgrid function does. What is the difference between <code>x_2d</code> and <code>y_2d</code>?\u00b6","text":""},{"location":"Week_4_Numpy/#3-indexing-in-numpy","title":"3. Indexing in Numpy\u00b6","text":"<p>Indexing in NumPy allows you to access and modify elements, rows, columns, or subarrays of an array. Basic indexing is similar to lists.</p>"},{"location":"Week_4_Numpy/#exercise-4-get-the-last-two-columns-of-y_2d-array","title":"Exercise 4: Get the last two columns of <code>y_2d</code> array.\u00b6","text":""},{"location":"Week_4_Numpy/#4-array-operations","title":"4. Array Operations\u00b6","text":"<p>There are a huge number of operations available on arrays. All the familiar arithemtic operators are applied on an element-by-element basis.</p>"},{"location":"Week_4_Numpy/#41-basic-math","title":"4.1 Basic Math\u00b6","text":""},{"location":"Week_4_Numpy/#visualizing-arrays-with-matplotlib","title":"Visualizing Arrays with Matplotlib\u00b6","text":"<p>It can be hard to work with big arrays without actually seeing anything with our eyes! We will now bring in Matplotib to start visualizating these arrays. For now we will just skim the surface of Matplotlib. Much more depth will be provided in the next chapter.</p>"},{"location":"Week_4_Numpy/#42-manipulating-array-dimensions","title":"4.2 Manipulating array dimensions\u00b6","text":""},{"location":"Week_4_Numpy/#43-broadcasting","title":"4.3 Broadcasting\u00b6","text":"<p>Broadcasting is an efficient way to multiply arrays of different sizes</p>"},{"location":"Week_4_Numpy/#exercise-5-what-is-the-difference-between-y-and-y_new-why-f-y-gives-an-error-but-f-y_new-doesnt","title":"Exercise 5: What is the difference between y and y_new? Why <code>f * y</code> gives an error, but <code>f * y_new</code> doesn't?\u00b6","text":""},{"location":"Week_4_Numpy/#44-reduction-operations","title":"4.4 Reduction Operations\u00b6","text":""},{"location":"Week_4_Numpy/#5-missing-data","title":"5. Missing data\u00b6","text":""},{"location":"Week_4_Numpy/#6-assignment","title":"6. Assignment\u00b6","text":""},{"location":"Week_4_Numpy/#61-create-two-2d-arrays-representing-coordinates-x-y","title":"6.1. Create two 2D arrays representing coordinates x, y\u00b6","text":"<p>Both should cover the range (-2, 2) and have 100 points in each direction</p>"},{"location":"Week_4_Numpy/#62-visualize-each-2d-array-using-pcolormesh","title":"6.2. Visualize each 2D array using <code>pcolormesh</code>\u00b6","text":"<p>Use the correct coordiantes for the x and y axes.</p>"},{"location":"Week_4_Numpy/#63-from-your-cartesian-coordinates-create-polar-coordinates-r-and-varphi","title":"6.3 From your cartesian coordinates, create polar coordinates $r$ and $\\varphi$:\u00b6","text":"<p>$r = \\sqrt{x^2 + y^2}$</p> <p>$\\varphi = atan2(y,x)$</p> <p>You will need to use numpy's <code>arctan2</code> function. Read its documentation.</p>"},{"location":"Week_4_Numpy/#64-visualize-r-and-varphi-on-the-2d-x-y-plane-using-pcolormesh","title":"6.4. Visualize $r$ and $\\varphi$ on the 2D  $x$ / $y$ plane using <code>pcolormesh</code>\u00b6","text":""},{"location":"Week_4_Numpy/#65-caclulate-the-quanity-f-cos24r-sin24varphi","title":"6.5 Caclulate the quanity $f = \\cos^2(4r) + \\sin^2(4\\varphi)$\u00b6","text":"<p>And plot it on the $x$ / $y$ plane</p>"},{"location":"Week_4_Numpy/#66-plot-the-mean-of-f-with-respect-to-the-x-axis","title":"6.6 Plot the mean of f with respect to the x axis\u00b6","text":"<p>as a function of y</p>"},{"location":"Week_4_Numpy/#67-plot-the-mean-of-f-with-respect-to-the-y-axis","title":"6.7 Plot the mean of f with respect to the y axis\u00b6","text":"<p>as a function of x</p>"},{"location":"Week_5_Pandas_Basics/","title":"Week 5: Working with Tabular Data in Python with Pandas","text":"<p>This week, you will learn how to work with tabular data in Python.</p> <p>Pandas is a an open source library providing high-performance, easy-to-use data structures and data analysis tools. Pandas is particularly suited to the analysis of tabular data, i.e. data that can can go into a table. In other words, if you can imagine the data in an Excel spreadsheet, then Pandas is the tool for the job.</p> <p>A recent analysis of questions from Stack Overflow showed that python is the fastest growing and most widely used programming language in the world (in developed countries).</p> <p>A follow-up analysis showed that this growth is driven by the data science packages such as numpy, matplotlib, and especially pandas.</p> <p>The exponential growth of pandas is due to the fact that it just works. It saves you time and helps you do science more efficiently and effictively.</p> <p>Let's start by importing pandas library.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[4]: Copied! <pre>from IPython.display import Image\nImage(url='https://storage.googleapis.com/lds-media/images/series-and-dataframe.width-1200.png')\n</pre> from IPython.display import Image Image(url='https://storage.googleapis.com/lds-media/images/series-and-dataframe.width-1200.png') Out[4]: In\u00a0[5]: Copied! <pre>s = pd.Series([1, 3, 5, np.nan, 6, 8])\n</pre> s = pd.Series([1, 3, 5, np.nan, 6, 8]) In\u00a0[7]: Copied! <pre>s\n</pre> s Out[7]: <pre>0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64</pre> <p>By default, Pandas creates a default RangeIndex starting from 0.</p> <p>Creating a Series by passing values and indices.</p> In\u00a0[9]: Copied! <pre>s = pd.Series([1, 3, 5, np.nan, 6, 8], index = ['A','B','C','D','E','F'])\ns\n</pre> s = pd.Series([1, 3, 5, np.nan, 6, 8], index = ['A','B','C','D','E','F']) s Out[9]: <pre>A    1.0\nB    3.0\nC    5.0\nD    NaN\nE    6.0\nF    8.0\ndtype: float64</pre> <p>Pandas has plotting functions that can easily visualize the data.</p> In\u00a0[11]: Copied! <pre>s.plot(kind = 'bar')\n</pre> s.plot(kind = 'bar') Out[11]: <pre>&lt;Axes: &gt;</pre> <p>Arithmetic operations and most numpy function can be applied to Series.</p> <p>An important point is that the Series keep their index during such operations.</p> In\u00a0[12]: Copied! <pre>np.sqrt(s)\n</pre> np.sqrt(s) Out[12]: <pre>A    1.000000\nB    1.732051\nC    2.236068\nD         NaN\nE    2.449490\nF    2.828427\ndtype: float64</pre> <p>We can also access the underlying index object if we need to:</p> In\u00a0[13]: Copied! <pre>s.index\n</pre> s.index Out[13]: <pre>Index(['A', 'B', 'C', 'D', 'E', 'F'], dtype='object')</pre> <p>We can get values back out using the index via the .loc attribute</p> In\u00a0[14]: Copied! <pre>s.loc['A']\n</pre> s.loc['A'] Out[14]: <pre>1.0</pre> <p>Or by raw position using .iloc:</p> In\u00a0[15]: Copied! <pre>s.iloc[0]\n</pre> s.iloc[0] Out[15]: <pre>1.0</pre> <p>We can pass a list or array to loc to get multiple rows back:</p> In\u00a0[21]: Copied! <pre>s.loc[['A','C']]\n</pre> s.loc[['A','C']] Out[21]: <pre>A    1.0\nC    5.0\ndtype: float64</pre> <p>We can also use the slice notation:</p> In\u00a0[20]: Copied! <pre>s.loc['A':'C']\n</pre> s.loc['A':'C']  Out[20]: <pre>A    1.0\nC    5.0\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[61]: Copied! <pre># Data: AQI levels and temperature in different cities\ndata = {\n    \"AQI\": [55, 75, 60, 80, np.NaN],\n    \"Temperature\": [68, 75, 64, 80, 85]\n}\n\n# Creating the DataFrame\ndf = pd.DataFrame(data, index = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"])\n</pre> # Data: AQI levels and temperature in different cities data = {     \"AQI\": [55, 75, 60, 80, np.NaN],     \"Temperature\": [68, 75, 64, 80, 85] }  # Creating the DataFrame df = pd.DataFrame(data, index = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"])  In\u00a0[62]: Copied! <pre>df\n</pre> df Out[62]: AQI Temperature New York 55.0 68 Los Angeles 75.0 75 Chicago 60.0 64 Houston 80.0 80 Phoenix NaN 85 In\u00a0[63]: Copied! <pre># You can set the style of the table\ndf.style.highlight_max()\n</pre> # You can set the style of the table df.style.highlight_max() Out[63]: AQI Temperature New York 55.000000 68 Los Angeles 75.000000 75 Chicago 60.000000 64 Houston 80.000000 80 Phoenix nan 85 <p>You may notice that Pandas handles missing data very elegantly, keeping track of it through all calculations.</p> <p>We can get a single column as a Series using python's getitem syntax on the DataFrame object.</p> In\u00a0[64]: Copied! <pre>df['AQI']\n</pre> df['AQI'] Out[64]: <pre>New York       55.0\nLos Angeles    75.0\nChicago        60.0\nHouston        80.0\nPhoenix         NaN\nName: AQI, dtype: float64</pre> <p>...or using attribute syntax.</p> In\u00a0[65]: Copied! <pre>df.AQI\n</pre> df.AQI Out[65]: <pre>New York       55.0\nLos Angeles    75.0\nChicago        60.0\nHouston        80.0\nPhoenix         NaN\nName: AQI, dtype: float64</pre> <p>If you want to get a specific row, use .loc</p> In\u00a0[66]: Copied! <pre>df.loc['New York']\n</pre> df.loc['New York'] Out[66]: <pre>AQI            55.0\nTemperature    68.0\nName: New York, dtype: float64</pre> In\u00a0[67]: Copied! <pre>df.iloc[2]\n</pre> df.iloc[2] Out[67]: <pre>AQI            60.0\nTemperature    64.0\nName: Chicago, dtype: float64</pre> <p>But we can also specify the column and row we want to access:</p> In\u00a0[68]: Copied! <pre>df.loc['New York','AQI']\n</pre> df.loc['New York','AQI'] Out[68]: <pre>55.0</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[28]: Copied! <pre>df.min()\n</pre> df.min() Out[28]: <pre>AQI            55.0\nTemperature    64.0\ndtype: float64</pre> In\u00a0[30]: Copied! <pre>df.idxmax()\n</pre> df.idxmax() Out[30]: <pre>AQI            Houston\nTemperature    Phoenix\ndtype: object</pre> In\u00a0[31]: Copied! <pre>df.mean()\n</pre> df.mean() Out[31]: <pre>AQI            67.5\nTemperature    74.4\ndtype: float64</pre> In\u00a0[32]: Copied! <pre>df.count()\n</pre> df.count() Out[32]: <pre>AQI            4\nTemperature    5\ndtype: int64</pre> In\u00a0[33]: Copied! <pre>df.std()\n</pre> df.std()  Out[33]: <pre>AQI            11.902381\nTemperature     8.561542\ndtype: float64</pre> In\u00a0[34]: Copied! <pre># describe function can print out the descriptive statistics easily!\ndf.describe()\n</pre> # describe function can print out the descriptive statistics easily! df.describe() Out[34]: AQI Temperature count 4.000000 5.000000 mean 67.500000 74.400000 std 11.902381 8.561542 min 55.000000 64.000000 25% 58.750000 68.000000 50% 67.500000 75.000000 75% 76.250000 80.000000 max 80.000000 85.000000 In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Add new row:</p> In\u00a0[69]: Copied! <pre>df.loc['New Brunswick'] = [64, 75]\n</pre> df.loc['New Brunswick'] = [64, 75] In\u00a0[70]: Copied! <pre>df\n</pre> df Out[70]: AQI Temperature New York 55.0 68 Los Angeles 75.0 75 Chicago 60.0 64 Houston 80.0 80 Phoenix NaN 85 New Brunswick 64.0 75 <p>Add new column:</p> In\u00a0[71]: Copied! <pre>df['Cloudy'] = [True, False, True, False, False, True]\n</pre> df['Cloudy'] = [True, False, True, False, False, True] In\u00a0[72]: Copied! <pre>df\n</pre> df Out[72]: AQI Temperature Cloudy New York 55.0 68 True Los Angeles 75.0 75 False Chicago 60.0 64 True Houston 80.0 80 False Phoenix NaN 85 False New Brunswick 64.0 75 True In\u00a0[73]: Copied! <pre>data = {\n    \"Latitude\": [40.7128, 34.0522, 41.8781, 29.7604, 33.4484],\n    \"Longitude\": [-74.0060, -118.2437, -87.6298, -95.3698, -112.0740]\n}\n\ndf_loc = pd.DataFrame(data, index = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"])\n</pre> data = {     \"Latitude\": [40.7128, 34.0522, 41.8781, 29.7604, 33.4484],     \"Longitude\": [-74.0060, -118.2437, -87.6298, -95.3698, -112.0740] }  df_loc = pd.DataFrame(data, index = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"])  In\u00a0[74]: Copied! <pre>df_loc\n</pre> df_loc Out[74]: Latitude Longitude New York 40.7128 -74.0060 Los Angeles 34.0522 -118.2437 Chicago 41.8781 -87.6298 Houston 29.7604 -95.3698 Phoenix 33.4484 -112.0740 <p>Combine df and df_loc:</p> In\u00a0[75]: Copied! <pre>df_join = df.join(df_loc)\n</pre> df_join = df.join(df_loc) In\u00a0[77]: Copied! <pre>df_join\n</pre> df_join Out[77]: AQI Temperature Cloudy Latitude Longitude New York 55.0 68 True 40.7128 -74.0060 Los Angeles 75.0 75 False 34.0522 -118.2437 Chicago 60.0 64 True 41.8781 -87.6298 Houston 80.0 80 False 29.7604 -95.3698 Phoenix NaN 85 False 33.4484 -112.0740 New Brunswick 64.0 75 True NaN NaN In\u00a0[79]: Copied! <pre>df_join = df.join(df_loc, how = 'right')\ndf_join\n</pre> df_join = df.join(df_loc, how = 'right') df_join Out[79]: AQI Temperature Cloudy Latitude Longitude New York 55.0 68 True 40.7128 -74.0060 Los Angeles 75.0 75 False 34.0522 -118.2437 Chicago 60.0 64 True 41.8781 -87.6298 Houston 80.0 80 False 29.7604 -95.3698 Phoenix NaN 85 False 33.4484 -112.0740 <p>Using concat to append new rows:</p> In\u00a0[80]: Copied! <pre>df_append = pd.concat([df,df_loc])\n</pre> df_append = pd.concat([df,df_loc]) In\u00a0[81]: Copied! <pre>df_append\n</pre> df_append Out[81]: AQI Temperature Cloudy Latitude Longitude New York 55.0 68.0 True NaN NaN Los Angeles 75.0 75.0 False NaN NaN Chicago 60.0 64.0 True NaN NaN Houston 80.0 80.0 False NaN NaN Phoenix NaN 85.0 False NaN NaN New Brunswick 64.0 75.0 True NaN NaN New York NaN NaN NaN 40.7128 -74.0060 Los Angeles NaN NaN NaN 34.0522 -118.2437 Chicago NaN NaN NaN 41.8781 -87.6298 Houston NaN NaN NaN 29.7604 -95.3698 Phoenix NaN NaN NaN 33.4484 -112.0740 <p>As you can see, pd.read_csv() has quite a few parameters. Don't be overwhelmed \u2013 most of these are optional arguments that allow you to specify exactly how your data file is structured and which part(s) you want to import. In particular, the sep parameter allows the user to specify the type of delimiter used in the file. The default is a comma, but you can actually pass other common delimiters (such as sep='\\t', which is a tab) to import other delimited files. The only required argument is a string specifying the filepath of your file.</p> In\u00a0[\u00a0]: Copied! <pre>pd.read_csv?\n</pre> pd.read_csv? <p>Download wildfire data <code>Spatial_Database_Big_Wildfires_US_all.csv</code> from Canvas, and upload the data to your current working directory.</p> In\u00a0[84]: Copied! <pre>df = pd.read_csv('Spatial_Database_Big_Wildfires_US_all.csv')\ndf.head()\n</pre> df = pd.read_csv('Spatial_Database_Big_Wildfires_US_all.csv') df.head() Out[84]: FOD_ID FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME 0 17 FS-1418878 FED FS-FIRESTAT USCAENF Eldorado National Forest NaN POWER POWER NaN ... 295.0 16823.0 G 38.523333 -120.211667 USFS CA 5 6005.0 Amador County 1 18 FS-1418881 FED FS-FIRESTAT USCAENF Eldorado National Forest BHA3 FREDS FREDS NaN ... 291.0 7700.0 G 38.780000 -120.260000 USFS CA 17 6017.0 El Dorado County 2 40 FS-1418920 FED FS-FIRESTAT USNCNCF National Forests in North Carolina BKC8 AUSTIN CREEK NaN NaN ... 44.0 125.0 D 36.001667 -81.590000 MISSING/NOT SPECIFIED NC 27 37027.0 Caldwell County 3 119 FS-1419153 FED FS-FIRESTAT USNENBF Nebraska National Forest BEW8 THOMPSON BUTTE NaN NaN ... 198.0 119.0 D 43.899167 -102.954722 USFS SD 103 46103.0 Pennington County 4 120 FS-1419156 FED FS-FIRESTAT USNENBF Nebraska National Forest BEW8 CHARLES DRAW NaN NaN ... 197.0 119.0 D 43.892778 -102.948056 USFS SD 103 46103.0 Pennington County <p>5 rows \u00d7 28 columns</p> In\u00a0[85]: Copied! <pre>df.columns\n</pre> df.columns Out[85]: <pre>Index(['FOD_ID', 'FPA_ID', 'SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM',\n       'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'FIRE_CODE',\n       'FIRE_NAME', 'MTBS_FIRE_NAME', 'COMPLEX_NAME', 'FIRE_YEAR',\n       'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n       'NWCG_CAUSE_CLASSIFICATION', 'NWCG_GENERAL_CAUSE',\n       'NWCG_CAUSE_AGE_CATEGORY', 'CONT_DATE', 'CONT_DOY', 'FIRE_SIZE',\n       'FIRE_SIZE_CLASS', 'LATITUDE', 'LONGITUDE', 'OWNER_DESCR', 'STATE',\n       'COUNTY', 'FIPS_CODE', 'FIPS_NAME'],\n      dtype='object')</pre> In\u00a0[86]: Copied! <pre>df = df.set_index('FOD_ID') #Set index\n</pre> df = df.set_index('FOD_ID') #Set index In\u00a0[87]: Copied! <pre>df[['DISCOVERY_DATE','DISCOVERY_DOY','DISCOVERY_TIME' ]]\n</pre> df[['DISCOVERY_DATE','DISCOVERY_DOY','DISCOVERY_TIME' ]] Out[87]: DISCOVERY_DATE DISCOVERY_DOY DISCOVERY_TIME FOD_ID 17 10/6/2004 280 1415.0 18 10/13/2004 287 1618.0 40 2/12/2005 43 1520.0 119 7/16/2005 197 1715.0 120 7/16/2005 197 1730.0 ... ... ... ... 400732975 8/9/2019 221 2134.0 400732976 3/1/2020 61 1330.0 400732977 5/13/2020 134 1300.0 400732982 8/17/2020 230 755.0 400732984 11/20/2020 325 1110.0 <p>60713 rows \u00d7 3 columns</p> In\u00a0[88]: Copied! <pre>df.groupby('STATE').FPA_ID.count().nlargest(10).plot(kind='bar', figsize=(12,6))\n</pre> df.groupby('STATE').FPA_ID.count().nlargest(10).plot(kind='bar', figsize=(12,6)) Out[88]: <pre>&lt;Axes: xlabel='STATE'&gt;</pre> In\u00a0[89]: Copied! <pre>df.groupby(df.STATE)\n</pre> df.groupby(df.STATE) Out[89]: <pre>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x30f804280&gt;</pre> <p>There is a shortcut for doing this with dataframes: you just pass the column name:</p> In\u00a0[90]: Copied! <pre>df.groupby('STATE')\n</pre> df.groupby('STATE') Out[90]: <pre>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x30d529a30&gt;</pre> In\u00a0[91]: Copied! <pre>gb = df.groupby('STATE')\ngb\n</pre> gb = df.groupby('STATE') gb Out[91]: <pre>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x30d0d0b80&gt;</pre> <p>The length tells us how many groups were found:</p> In\u00a0[92]: Copied! <pre>len(gb)\n</pre> len(gb) Out[92]: <pre>50</pre> <p>All of the groups are available as a dictionary via the <code>.groups</code> attribute:</p> In\u00a0[93]: Copied! <pre>groups = gb.groups\nlen(groups)\n</pre> groups = gb.groups len(groups) Out[93]: <pre>50</pre> In\u00a0[94]: Copied! <pre>groups['NJ']\n</pre> groups['NJ'] Out[94]: <pre>Index([   247140,    384407,    578890,    579164,    579296,    580135,\n          580956,    581165,    582330,    582680,\n       ...\n       400271442, 400389794, 400482043, 400528139, 400531379, 400587134,\n       400594776, 400602511, 400613352, 400632920],\n      dtype='int64', name='FOD_ID', length=111)</pre> In\u00a0[95]: Copied! <pre>for key, group in gb:\n    display(group.head())\n    print(f'The key is \"{key}\"')\n    break\n</pre> for key, group in gb:     display(group.head())     print(f'The key is \"{key}\"')     break FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 6689 FS-1431539 FED FS-FIRESTAT USAKTNF Tongass National Forest BRD1 MUSKEG NaN NaN 2005 ... 126.0 305.0 E 59.087222 -135.441389 STATE OR PRIVATE AK 220 2220.0 Sitka City and Borough 109456 FS-334441 FED FS-FIRESTAT USAKTNF Tongass National Forest NaN MILL NaN NaN 1998 ... 189.0 118.0 D 55.681667 -132.615000 STATE OR PRIVATE AK NaN NaN NaN 147361 FS-374211 FED FS-FIRESTAT USAKCGF Chugach National Forest NaN KENAI LAKE KENAI LAKE NaN 2001 ... 188.0 3260.0 F 60.410278 -149.473611 USFS AK NaN NaN NaN 174677 W-374459 FED DOI-WFMI USAKAKA Alaska Regional Office B391 B391 532391 NaN 1995 ... 226.0 2850.0 F 66.832700 -160.736100 TRIBAL AK NaN NaN NaN 213301 W-36457 FED DOI-WFMI USAKAKD Alaska Fire Service A029 203029 NaN NaN 1992 ... 126.0 170.0 D 57.065900 -154.085700 BIA AK NaN NaN NaN <p>5 rows \u00d7 27 columns</p> <pre>The key is \"AK\"\n</pre> <p>And you can get a specific group by key.</p> In\u00a0[96]: Copied! <pre>gb.get_group('NJ')\n</pre> gb.get_group('NJ') Out[96]: FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 247140 W-234848 FED DOI-WFMI USPADWP Delaware Water Gap National Recreation Area NaN WORTHINGTO WORTHINGTO NaN 1999 ... 102.0 623.0 E 40.995895 -75.120000 NPS NJ NaN NaN NaN 384407 FWS-2007NJERRDFR2 FED FWS-FMIS USNJERR Edwin B. Forsythe National Wildlife Refuge DFR2 NJ NJFFS WF ASSIST WARREN GROVE WARREN GROVE NaN 2007 ... 141.0 17050.0 G 39.707500 -74.309722 STATE NJ NaN NaN NaN 578890 SFO-2006NJDEPA032704 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN NaN NaN 2006 ... NaN 104.0 D 40.304400 -74.201100 PRIVATE NJ Middlesex 34023.0 Middlesex County 579164 SFO-2006NJDEPB012703 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN RARITAN CENTER NaN NaN 2006 ... NaN 450.0 E 40.296100 -74.214000 PRIVATE NJ Middlesex 34023.0 Middlesex County 579296 SFO-2006NJDEPB032108 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN SUNRISE LAKE NaN NaN 2006 ... NaN 136.0 D 39.482800 -74.510900 STATE NJ Burlington 34005.0 Burlington County ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 400587134 SFO-2020NJDEPA03-200223155509 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN NaN NaN 2020 ... NaN 103.0 D 40.970480 -75.117710 MISSING/NOT SPECIFIED NJ Warren 34041.0 Warren County 400594776 SFO-2020NJDEPC03-200409234633 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN SPLIT DITCH NaN 2020 ... NaN 1518.0 F 39.312640 -75.090240 MISSING/NOT SPECIFIED NJ Cumberland 34011.0 Cumberland County 400602511 SFO-2020NJDEPC06-200519235337 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN BIG TIMBER NaN 2020 ... NaN 2107.0 F 39.651250 -74.892050 MISSING/NOT SPECIFIED NJ Camden 34007.0 Camden County 400613352 SFO-2020NJDEPB09-200709201959 NONFED ST-NASF USNJNJS New Jersey Forest Fire Service NaN NaN NaN NaN 2020 ... NaN 204.0 D 40.111570 -74.412330 MISSING/NOT SPECIFIED NJ Ocean 34029.0 Ocean County 400632920 ICS209_2019_10720324 INTERAGCY IA-ICS209 USNJNJS New Jersey Forest Fire Service NaN SPRING HILL FIRE SPRING HILL FIRE NaN 2019 ... NaN 11638.0 G 39.770000 -74.450000 MISSING/NOT SPECIFIED NJ Burlington 34005.0 Burlington County <p>111 rows \u00d7 27 columns</p> <p>By default, the operation is applied to every column. That's usually not what we want. We can use both <code>.</code> or <code>[]</code> syntax to select a specific column to operate on. Then we get back a series.</p> In\u00a0[121]: Copied! <pre># Find out the 10 states with biggest fire size. \n\ngb.FIRE_SIZE.max().nlargest(10)\n</pre> # Find out the 10 states with biggest fire size.   gb.FIRE_SIZE.max().nlargest(10) Out[121]: <pre>STATE  FIRE_YEAR\nOK     2017         662700.0\nAK     1997         606945.0\nCA     2020         589368.0\nOR     2012         558198.3\nAZ     2011         538049.0\nAK     2004         537627.0\n       2009         517078.0\nOR     2002         499945.0\nTX     2006         479549.0\nNV     2018         416821.2\nName: FIRE_SIZE, dtype: float64</pre> <p>There are shortcuts for common aggregation functions:</p> In\u00a0[99]: Copied! <pre>gb.FIRE_SIZE.max().nlargest(10)\n</pre> gb.FIRE_SIZE.max().nlargest(10) Out[99]: <pre>STATE\nOK    662700.0\nAK    606945.0\nCA    589368.0\nOR    558198.3\nAZ    538049.0\nTX    479549.0\nNV    416821.2\nID    367785.0\nUT    357185.0\nGA    309200.0\nName: FIRE_SIZE, dtype: float64</pre> In\u00a0[100]: Copied! <pre>gb.FIRE_SIZE.mean().nlargest(10)\n</pre> gb.FIRE_SIZE.mean().nlargest(10) Out[100]: <pre>STATE\nAK    16440.375603\nNV     6138.109191\nOR     5595.310524\nWA     5071.806130\nID     4657.836385\nCA     4086.919845\nMT     3571.759463\nAZ     3084.376006\nUT     2874.009960\nCO     2809.130881\nName: FIRE_SIZE, dtype: float64</pre> In\u00a0[103]: Copied! <pre>gb.FIRE_SIZE.sum().nlargest(10).plot(kind='bar')\n</pre> gb.FIRE_SIZE.sum().nlargest(10).plot(kind='bar') Out[103]: <pre>&lt;Axes: xlabel='STATE'&gt;</pre> In\u00a0[104]: Copied! <pre># Find out unique values of fire cause classification. \n\ndf['NWCG_CAUSE_CLASSIFICATION'].unique()\n</pre> # Find out unique values of fire cause classification.   df['NWCG_CAUSE_CLASSIFICATION'].unique() Out[104]: <pre>array(['Human', 'Natural', 'Missing data/not specified/undetermined'],\n      dtype=object)</pre> In\u00a0[105]: Copied! <pre># Find out unique values of fire general cause. \n\ndf['NWCG_GENERAL_CAUSE'].unique()\n</pre> # Find out unique values of fire general cause.   df['NWCG_GENERAL_CAUSE'].unique() Out[105]: <pre>array(['Equipment and vehicle use',\n       'Power generation/transmission/distribution',\n       'Debris and open burning', 'Natural',\n       'Missing data/not specified/undetermined',\n       'Recreation and ceremony', 'Smoking',\n       'Railroad operations and maintenance', 'Arson/incendiarism',\n       'Fireworks', 'Other causes', 'Misuse of fire by a minor',\n       'Firearms and explosives use'], dtype=object)</pre> In\u00a0[106]: Copied! <pre># Find out the 10 leading causes of fires. \n\ndf.groupby('NWCG_GENERAL_CAUSE').count()['FPA_ID'].nlargest(10).plot(kind = 'bar')\n</pre> # Find out the 10 leading causes of fires.   df.groupby('NWCG_GENERAL_CAUSE').count()['FPA_ID'].nlargest(10).plot(kind = 'bar') Out[106]: <pre>&lt;Axes: xlabel='NWCG_GENERAL_CAUSE'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[108]: Copied! <pre>gb = df.groupby(['STATE','FIRE_YEAR'])\nlen(gb)\n</pre> gb = df.groupby(['STATE','FIRE_YEAR']) len(gb) Out[108]: <pre>1231</pre> In\u00a0[109]: Copied! <pre>list(gb.groups.keys())[:100:10]\n</pre> list(gb.groups.keys())[:100:10] Out[109]: <pre>[('AK', 1992),\n ('AK', 2002),\n ('AK', 2012),\n ('AL', 1993),\n ('AL', 2003),\n ('AL', 2013),\n ('AR', 1994),\n ('AR', 2004),\n ('AR', 2014),\n ('AZ', 1995)]</pre> In\u00a0[110]: Copied! <pre>gb.FIRE_SIZE.sum()\n</pre> gb.FIRE_SIZE.sum() Out[110]: <pre>STATE  FIRE_YEAR\nAK     1992         141007.000\n       1993         684669.800\n       1994         259901.600\n       1995          42526.000\n       1996         596706.400\n                       ...    \nWY     2016         254804.700\n       2017         118803.020\n       2018         220718.000\n       2019          41022.200\n       2020         285791.255\nName: FIRE_SIZE, Length: 1231, dtype: float64</pre> In\u00a0[111]: Copied! <pre>### Select group with multiple index must use tuple!\ngb.get_group(('CA', 2003))\n</pre> ### Select group with multiple index must use tuple! gb.get_group(('CA', 2003)) Out[111]: FPA_ID SOURCE_SYSTEM_TYPE SOURCE_SYSTEM NWCG_REPORTING_UNIT_ID NWCG_REPORTING_UNIT_NAME FIRE_CODE FIRE_NAME MTBS_FIRE_NAME COMPLEX_NAME FIRE_YEAR ... CONT_DOY FIRE_SIZE FIRE_SIZE_CLASS LATITUDE LONGITUDE OWNER_DESCR STATE COUNTY FIPS_CODE FIPS_NAME FOD_ID 157551 FS-385211 FED FS-FIRESTAT USCAINF Inyo National Forest NaN DEXTER DEXTER WFU NaN 2003 ... 286.0 2515.0 F 37.831389 -118.795000 USFS CA NaN NaN NaN 158728 FS-386431 FED FS-FIRESTAT USCAPNF Plumas National Forest 4300 ROWLAND NaN NaN 2003 ... 163.0 114.0 D 39.951389 -120.068889 USFS CA NaN NaN NaN 159533 FS-387254 FED FS-FIRESTAT USCASTF Stanislaus National Forest 7648 MUDD MUD WFU MUD COMPLEX 2003 ... 300.0 4102.0 F 38.424722 -119.961111 USFS CA NaN NaN NaN 159534 FS-387255 FED FS-FIRESTAT USCASTF Stanislaus National Forest 5555 WHITT WHITT MUD COMPLEX 2003 ... 300.0 1014.0 F 38.378056 -119.999722 USFS CA NaN NaN NaN 160202 FS-388122 FED FS-FIRESTAT USCALPF Los Padres National Forest 2996 DEL VENTURI NaN NaN 2003 ... 225.0 861.0 E 36.071111 -121.390000 MISSING/NOT SPECIFIED CA NaN NaN NaN ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 15000827 ICS209_2003_CA-KRN-37559 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN HILLSIDE UNNAMED NaN 2003 ... 198.0 835.0 E 35.168056 -118.468056 MISSING/NOT SPECIFIED CA KERN 6029.0 Kern County 15000828 ICS209_2003_CA-KRN-38766 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN MERCURY UNNAMED NaN 2003 ... 203.0 340.0 E 35.200556 -118.518611 MISSING/NOT SPECIFIED CA KERN 6029.0 Kern County 15000829 ICS209_2003_CA-LAC-03004054 INTERAGCY IA-ICS209 USCALAC Los Angeles County Fire Department NaN AIRPORT NaN NaN 2003 ... 8.0 245.0 D 33.407778 -118.402500 MISSING/NOT SPECIFIED CA LOS ANGELES 6037.0 Los Angeles County 201940026 ICS209_2003-CA-KRN-0333259 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN TEJON TEJON NaN 2003 ... 183.0 1155.0 F 34.871389 -118.882778 MISSING/NOT SPECIFIED CA Kern 6029.0 Kern County 400280041 ICS209_2003_CA-KRN-33853 INTERAGCY IA-ICS209 USCAKRN Kern County Fire Department NaN GRAPEVINE GRAPEVINE NaN 2003 ... NaN 1830.0 F 34.916944 -118.918333 MISSING/NOT SPECIFIED CA Kern 6029.0 Kern County <p>210 rows \u00d7 27 columns</p> In\u00a0[112]: Copied! <pre>### Find out the largest fire in CA, 2020\ndf.loc[gb['FIRE_SIZE'].idxmax().loc['CA',2020]]\n</pre> ### Find out the largest fire in CA, 2020 df.loc[gb['FIRE_SIZE'].idxmax().loc['CA',2020]] Out[112]: <pre>FPA_ID                           IRW-2020-CAMNF-000730\nSOURCE_SYSTEM_TYPE                           INTERAGCY\nSOURCE_SYSTEM                                 IA-IRWIN\nNWCG_REPORTING_UNIT_ID                         USCAMNF\nNWCG_REPORTING_UNIT_NAME     Mendocino National Forest\nFIRE_CODE                                         NFP4\nFIRE_NAME                                          DOE\nMTBS_FIRE_NAME                          AUGUST COMPLEX\nCOMPLEX_NAME                            AUGUST COMPLEX\nFIRE_YEAR                                         2020\nDISCOVERY_DATE                               8/16/2020\nDISCOVERY_DOY                                      229\nDISCOVERY_TIME                                     NaN\nNWCG_CAUSE_CLASSIFICATION                      Natural\nNWCG_GENERAL_CAUSE                             Natural\nNWCG_CAUSE_AGE_CATEGORY                            NaN\nCONT_DATE                                   11/11/2020\nCONT_DOY                                         316.0\nFIRE_SIZE                                     589368.0\nFIRE_SIZE_CLASS                                      G\nLATITUDE                                     39.765255\nLONGITUDE                                  -122.672914\nOWNER_DESCR                                       USFS\nSTATE                                               CA\nCOUNTY                                           Glenn\nFIPS_CODE                                       6021.0\nFIPS_NAME                                 Glenn County\nName: 400629554, dtype: object</pre> In\u00a0[113]: Copied! <pre># Plot the number of wildfires every year and associated causes.\n\ndf.groupby(['FIRE_YEAR','NWCG_CAUSE_CLASSIFICATION']).FPA_ID.count().unstack('NWCG_CAUSE_CLASSIFICATION').plot(kind = 'bar', stacked = True)\n</pre> # Plot the number of wildfires every year and associated causes.  df.groupby(['FIRE_YEAR','NWCG_CAUSE_CLASSIFICATION']).FPA_ID.count().unstack('NWCG_CAUSE_CLASSIFICATION').plot(kind = 'bar', stacked = True) Out[113]: <pre>&lt;Axes: xlabel='FIRE_YEAR'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Week_5_Pandas_Basics/#week-5-working-with-tabular-data-in-python-with-pandas","title":"Week 5: Working with Tabular Data in Python with Pandas\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#pandas-capabilities-from-the-pandas-website","title":"Pandas capabilities (from the Pandas website):\u00b6","text":"<ul> <li>A fast and efficient DataFrame object for data manipulation with integrated indexing;</li> <li>Tools for reading and writing data between in-memory data structures and different formats: CSV and text files, Microsoft Excel, SQL databases, and the fast HDF5 format;</li> <li>Intelligent data alignment and integrated handling of missing data: gain automatic label-based alignment in computations and easily manipulate messy data into an orderly form;</li> <li>Flexible reshaping and pivoting of data sets;</li> <li>Intelligent label-based slicing, fancy indexing, and subsetting of large data sets;</li> <li>Columns can be inserted and deleted from data structures for size mutability;</li> <li>Aggregating or transforming data with a powerful group by engine allowing split-apply-combine operations on data sets;</li> <li>High performance merging and joining of data sets;</li> <li>Hierarchical axis indexing provides an intuitive way of working with high-dimensional data in a lower-dimensional data structure;</li> <li>Time series-functionality: date range generation and frequency conversion, moving window statistics, moving window linear regressions, date shifting and lagging. Even create domain-specific time offsets and join time series without losing data;</li> <li>Highly optimized for performance, with critical code paths written in Cython or C.</li> <li>Python with pandas is in use in a wide variety of academic and commercial domains, including Finance, Neuroscience, Economics, Statistics, Advertising, Web Analytics, and more.</li> </ul>"},{"location":"Week_5_Pandas_Basics/#1-basic-pandas","title":"1. Basic Pandas\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#11-basic-data-structures-in-pandas","title":"1.1 Basic data structures in Pandas:\u00b6","text":"<p>Pandas provides two types of classes for handling data:</p> <ol> <li>Series: a one-dimensional labeled array holding data of any type.</li> <li>DataFrame: a two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns.</li> </ol> <p>You can think of Pandas Dataframe as an Excel spreadsheet, and Series as one column of the the spreadsheet. Multiple series can be combined as a DataFrame.</p>"},{"location":"Week_5_Pandas_Basics/#12-pandas-series","title":"1.2 Pandas Series\u00b6","text":"<p>Creating a Series by passing a list of values.</p>"},{"location":"Week_5_Pandas_Basics/#exercise-1-create-a-pandas-series-with-values-ranging-from-0-to-6-and-label-the-index-as-the-day-of-week-starting-from-sunday-to-saturday","title":"Exercise 1: Create a Pandas series with values ranging from 0 to 6, and label the index as the day of week, starting from 'Sunday' to 'Saturday'.\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#13-indexing","title":"1.3 Indexing\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#exercise-2-print-out-the-last-two-elements-of-series-s-hint-using-slice-notation-and-iloc","title":"Exercise 2: Print out the last two elements of Series s (hint: using slice notation and iloc).\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#14-pandas-data-structure-dataframe","title":"1.4 Pandas data structure: DataFrame\u00b6","text":"<p>A more useful Pandas data structure is the DataFrame. A DataFrame is basically a bunch of series that share the same index. It's a lot like a table in a spreadsheet.</p> <p>Below we create a DataFrame.</p>"},{"location":"Week_5_Pandas_Basics/#15-index-with-pandas-dataframe","title":"1.5 Index with Pandas DataFrame:\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#exercise-3-print-out-the-temperature-of-chicago-and-los-angeles","title":"Exercise 3: Print out the temperature of Chicago and Los Angeles.\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#16-basic-statistics-with-pandas","title":"1.6 Basic statistics with Pandas:\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#exercise-4-print-out-the-city-with-minimum-temperature","title":"Exercise 4: Print out the city with minimum temperature.\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#17-modifying-pandas-dataframe","title":"1.7 Modifying Pandas DataFrame\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#2-advanced-pandas","title":"2. Advanced Pandas\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#21-importing-tabular-data-in-pandas","title":"2.1 Importing tabular data in pandas\u00b6","text":"<p>In Pandas, the most commonly used function to import tabular data is <code>read_csv</code> function</p>"},{"location":"Week_5_Pandas_Basics/#22-groupby-methods","title":"2.2 Groupby methods\u00b6","text":"<p><code>groupby</code> is an amazingly powerful function in pandas, but it is also complicated to use and understand. The point of this section is to make you feel confident in using <code>groupby</code>.</p>"},{"location":"Week_5_Pandas_Basics/#an-example","title":"An Example:\u00b6","text":"<p>Question: Find out the top 10 states with largest number of wildfires.</p> <p>This is an example of a \"one-liner\" that you can accomplish with groupby.</p>"},{"location":"Week_5_Pandas_Basics/#what-happened","title":"What Happened?\u00b6","text":"<p>Let's break apart this operation a bit. The workflow with <code>groubpy</code> can be divided into three general steps:</p> <ol> <li><p>Split: Partition the data into different groups based on some criterion.</p> </li> <li><p>Apply: Do some caclulation within each group. Different types of \"apply\" steps might be</p> <ul> <li> Aggregation: Get the mean or max within the group. </li> <li> Transformation: Normalize all the values within a group. </li> <li> Filtration: Eliminate some groups based on a criterion. </li> </ul> </li> <li><p>Combine: Put the results back together into a single object.</p> </li> </ol>"},{"location":"Week_5_Pandas_Basics/#the-groupby-method","title":"The <code>groupby</code> method\u00b6","text":"<p>Both <code>Series</code> and <code>DataFrame</code> objects have a groupby method. It accepts a variety of arguments, but the simplest way to think about it is that you pass another series, whose unique values are used to split the original object into different groups.</p>"},{"location":"Week_5_Pandas_Basics/#the-groubby-object","title":"The <code>GroubBy</code> object\u00b6","text":"<p>When we call, <code>groupby</code> we get back a <code>GroupBy</code> object:</p>"},{"location":"Week_5_Pandas_Basics/#23-iterating-and-selecting-groups","title":"2.3 Iterating and selecting groups\u00b6","text":"<p>You can loop through the groups if you want.</p>"},{"location":"Week_5_Pandas_Basics/#24-aggregation","title":"2.4 Aggregation\u00b6","text":"<p>Now that we know how to create a <code>GroupBy</code> object, let's learn how to do aggregation on it.</p> <p>One way us to use the <code>.aggregate</code> method, which accepts another function as its argument. The result is automatically combined into a new dataframe with the group key as the index.</p>"},{"location":"Week_5_Pandas_Basics/#exercise-5-plot-the-number-of-wildfires-every-year","title":"Exercise 5: Plot the number of wildfires every year.\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#exercise-6-plot-the-number-of-wildfires-in-ca-every-year","title":"Exercise 6: Plot the number of wildfires in CA every year.\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#25-groupby-multiple-index","title":"2.5 Groupby multiple index\u00b6","text":""},{"location":"Week_5_Pandas_Basics/#exercise-7-plot-the-number-of-wildfires-in-nj-every-year-and-associated-causes","title":"Exercise 7: Plot the number of wildfires in NJ every year and associated causes.\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/","title":"Week 6: Correlation and Regressions in Python","text":"<p>This week, you will learn how to conduct correlation and linear regression analysis in Python</p> <p>In Environmental Sciences, we often come up with the ideas of exploring the relationship between two sets of measurements (e.g., How is air quality correlated with temperature?). The first step of the exploratory correlation analysis is to link the two datasets by matching the measurements taken at the same time and same place. Matching datasets has been a pain in Excel, but with the indexing functionality in Pandas, it is convenient to link datasets.</p> In\u00a0[1]: Copied! <pre>#Getting all the packages we need: \nimport pandas as pd \nimport numpy as np\n</pre> #Getting all the packages we need:  import pandas as pd  import numpy as np In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Here we use an example to explore the correlation between ozone air quality and temperature at Millbrook Site in New York. We have two datasets: (1) Daily ozone measurements at this site (https://www.epa.gov/outdoor-air-quality-data/download-daily-data); (2) NOAA weather station data at this site (https://www.ncdc.noaa.gov/data-access/land-based-station-data).</p> <p>First, let's read in these two datasets using Pandas <code>read_csv</code> fuction:</p> In\u00a0[9]: Copied! <pre>df_AQS = pd.read_csv('Millbrook_NY_daily_ozone_2022.csv', parse_dates = ['Date'])\n</pre> df_AQS = pd.read_csv('Millbrook_NY_daily_ozone_2022.csv', parse_dates = ['Date']) In\u00a0[10]: Copied! <pre>df_AQS.head()\n</pre> df_AQS.head() Out[10]: Date Source Site ID POC Daily Max 8-hour Ozone Concentration Units Daily AQI Value Local Site Name Daily Obs Count Percent Complete ... AQS Parameter Description Method Code CBSA Code CBSA Name State FIPS Code State County FIPS Code County Site Latitude Site Longitude 0 2022-01-15 AQS 360270007 1 0.034 ppm 31 MILLBROOK 17 100.0 ... Ozone 87 35620 New York-Newark-Jersey City, NY-NJ-PA 36 New York 27 Dutchess 41.78555 -73.74136 1 2022-01-16 AQS 360270007 1 0.037 ppm 34 MILLBROOK 17 100.0 ... Ozone 87 35620 New York-Newark-Jersey City, NY-NJ-PA 36 New York 27 Dutchess 41.78555 -73.74136 2 2022-01-17 AQS 360270007 1 0.038 ppm 35 MILLBROOK 17 100.0 ... Ozone 87 35620 New York-Newark-Jersey City, NY-NJ-PA 36 New York 27 Dutchess 41.78555 -73.74136 3 2022-01-18 AQS 360270007 1 0.042 ppm 39 MILLBROOK 17 100.0 ... Ozone 87 35620 New York-Newark-Jersey City, NY-NJ-PA 36 New York 27 Dutchess 41.78555 -73.74136 4 2022-01-19 AQS 360270007 1 0.029 ppm 27 MILLBROOK 17 100.0 ... Ozone 87 35620 New York-Newark-Jersey City, NY-NJ-PA 36 New York 27 Dutchess 41.78555 -73.74136 <p>5 rows \u00d7 21 columns</p> In\u00a0[11]: Copied! <pre>df_weather = pd.read_csv('Millbrook_NY_daily_weather.csv', parse_dates = ['LST_DATE'])\n</pre> df_weather = pd.read_csv('Millbrook_NY_daily_weather.csv', parse_dates = ['LST_DATE']) In\u00a0[12]: Copied! <pre>df_weather.head()\n</pre> df_weather.head() Out[12]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 0 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 ... 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 1 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 ... 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 ... 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 3 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 ... 0.220 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 4 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 ... 0.213 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN <p>5 rows \u00d7 29 columns</p> In\u00a0[13]: Copied! <pre>df_weather.columns\n</pre> df_weather.columns Out[13]: <pre>Index(['LST_DATE', 'WBANNO', 'CRX_VN', 'LONGITUDE', 'LATITUDE', 'T_DAILY_MAX',\n       'T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_AVG', 'P_DAILY_CALC',\n       'SOLARAD_DAILY', 'SUR_TEMP_DAILY_TYPE', 'SUR_TEMP_DAILY_MAX',\n       'SUR_TEMP_DAILY_MIN', 'SUR_TEMP_DAILY_AVG', 'RH_DAILY_MAX',\n       'RH_DAILY_MIN', 'RH_DAILY_AVG', 'SOIL_MOISTURE_5_DAILY',\n       'SOIL_MOISTURE_10_DAILY', 'SOIL_MOISTURE_20_DAILY',\n       'SOIL_MOISTURE_50_DAILY', 'SOIL_MOISTURE_100_DAILY',\n       'SOIL_TEMP_5_DAILY', 'SOIL_TEMP_10_DAILY', 'SOIL_TEMP_20_DAILY',\n       'SOIL_TEMP_50_DAILY', 'SOIL_TEMP_100_DAILY', 'Unnamed: 28'],\n      dtype='object')</pre> <p>Next, we need to merge the two dataframes by matching the date and location. In this dataset, as there is only one site, we don't need to match the location, so we just need to match the date. Set the <code>Date</code> column as index.</p> In\u00a0[14]: Copied! <pre>df_AQS = df_AQS.set_index('Date')\ndf_weather = df_weather.set_index('LST_DATE')\n</pre> df_AQS = df_AQS.set_index('Date') df_weather = df_weather.set_index('LST_DATE') <p>If we only need to add selected columns in <code>df_weather</code> to  <code>df_AQS</code>, we just assign the new columns to <code>df_AQS</code>. For single column:</p> In\u00a0[15]: Copied! <pre>df_AQS['T_DAILY_MEAN'] = df_weather['T_DAILY_MEAN']\n</pre> df_AQS['T_DAILY_MEAN'] = df_weather['T_DAILY_MEAN'] <p>For multiple columns:</p> In\u00a0[16]: Copied! <pre>df_AQS[['T_DAILY_MAX','T_DAILY_MIN','RH_DAILY_MAX']] = df_weather[['T_DAILY_MAX','T_DAILY_MIN','RH_DAILY_MAX']]\n</pre> df_AQS[['T_DAILY_MAX','T_DAILY_MIN','RH_DAILY_MAX']] = df_weather[['T_DAILY_MAX','T_DAILY_MIN','RH_DAILY_MAX']] <p>Now you should see <code>df_AQS</code> has three new columns with temperature. We've merged these two datasets!</p> In\u00a0[17]: Copied! <pre>df_AQS\n</pre> df_AQS Out[17]: Source Site ID POC Daily Max 8-hour Ozone Concentration Units Daily AQI Value Local Site Name Daily Obs Count Percent Complete AQS Parameter Code ... State FIPS Code State County FIPS Code County Site Latitude Site Longitude T_DAILY_MEAN T_DAILY_MAX T_DAILY_MIN RH_DAILY_MAX Date 2022-01-15 AQS 360270007 1 0.034 ppm 31 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 -14.7 -11.6 -17.9 60.0 2022-01-16 AQS 360270007 1 0.037 ppm 34 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 -10.4 -1.1 -19.7 87.1 2022-01-17 AQS 360270007 1 0.038 ppm 35 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 2.0 5.2 -1.1 87.1 2022-01-18 AQS 360270007 1 0.042 ppm 39 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 -4.8 0.3 -9.9 66.3 2022-01-19 AQS 360270007 1 0.029 ppm 27 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 -2.4 5.8 -10.6 76.8 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-26 AQS 360270007 1 0.033 ppm 31 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 -6.8 -2.4 -11.1 70.9 2022-12-27 AQS 360270007 1 0.030 ppm 28 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 -4.4 -0.8 -8.0 81.9 2022-12-28 AQS 360270007 1 0.027 ppm 25 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 0.7 7.4 -6.1 80.8 2022-12-29 AQS 360270007 1 0.030 ppm 28 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 4.4 10.7 -1.8 74.3 2022-12-30 AQS 360270007 1 0.028 ppm 26 MILLBROOK 17 100.0 44201 ... 36 New York 27 Dutchess 41.78555 -73.74136 10.7 16.6 4.9 68.5 <p>339 rows \u00d7 24 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Scatter plots allow you to quickly see if two variables are positively, negatively, or not correlated at all. You can directly use the plotting functions in Pandas:</p> In\u00a0[18]: Copied! <pre>df_AQS.plot.scatter('T_DAILY_MEAN','Daily Max 8-hour Ozone Concentration')\n</pre> df_AQS.plot.scatter('T_DAILY_MEAN','Daily Max 8-hour Ozone Concentration') Out[18]: <pre>&lt;Axes: xlabel='T_DAILY_MEAN', ylabel='Daily Max 8-hour Ozone Concentration'&gt;</pre> <p>The Pearson correlation coefficient r is calculated using the following formula:</p> <p>$$ r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}} $$</p> <p>where $x_i$ and $y_i$ are individual data points for variables $x$ and $y$; $\\bar{x}$ and $\\bar{y}$ are the means of $x$ and $y$; $n$ is the number of data points.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>You may find it a pain to calculate R value by hand. You just need to do it once! In Pandas, you can directly calculate Pearson Correlation Coefficient using the <code>corr</code> function.</p> In\u00a0[19]: Copied! <pre>pearson_corr = df_AQS['T_DAILY_MEAN'].corr(df_AQS['Daily Max 8-hour Ozone Concentration'], method='pearson')\n</pre> pearson_corr = df_AQS['T_DAILY_MEAN'].corr(df_AQS['Daily Max 8-hour Ozone Concentration'], method='pearson') In\u00a0[20]: Copied! <pre>pearson_corr\n</pre> pearson_corr Out[20]: <pre>np.float64(0.3047365963742261)</pre> <p>Check if the value is the same as the r value you calculated in Exercise 2.</p> <p>In Week 2, we learned that rank-based statistics are more appropriate for censored datasets with non-detects.</p> <p>The Spearman rank correlation coefficient (denoted as \u03c1) is a non-parametric measure of the strength and direction of the monotonic relationship between two ranked variables. Spearman correlation works on the ranks of the data, not the actual values. Each data point is assigned a rank, and the correlation is calculated based on these ranks. Like Pearson correlation, the Spearman correlation coefficient ranges from -1 to 1:</p> <ul> <li>\u03c1=1: Perfect positive monotonic relationship. As one variable increases, the other increases as well.</li> <li>\u03c1=\u22121: Perfect negative monotonic relationship. As one variable increases, the other decreases.</li> <li>\u03c1=0: No monotonic relationship between the variables.</li> </ul> <p>Spearman rank correlation coefficient does not assume normality or linearity. It can be applied to ordinal, interval, or ratio data. Since Spearman uses ranks, it is more robust to outliers and non-detects compared to Pearson correlation.</p> <p>To calculate the Spearman rank correlation coefficient in Pandas, we just simply need to change the method argument to 'spearman':</p> In\u00a0[21]: Copied! <pre>spearman_corr = df_AQS['T_DAILY_MEAN'].corr(df_AQS['Daily Max 8-hour Ozone Concentration'], method='spearman')\n</pre> spearman_corr = df_AQS['T_DAILY_MEAN'].corr(df_AQS['Daily Max 8-hour Ozone Concentration'], method='spearman') In\u00a0[22]: Copied! <pre>spearman_corr\n</pre> spearman_corr Out[22]: <pre>np.float64(0.3031817198225167)</pre> In\u00a0[23]: Copied! <pre># Calculate Pearson correlation matrix for multiple numeric columns\ncorr_matrix = df_AQS[['T_DAILY_MAX','T_DAILY_MIN','T_DAILY_MEAN','Daily Max 8-hour Ozone Concentration']].corr(method='pearson', numeric_only = True)\n\n# We can visualize the correlation metrics by setting the style of the output.\ncorr_matrix.style.background_gradient(cmap='Blues')\n</pre> # Calculate Pearson correlation matrix for multiple numeric columns corr_matrix = df_AQS[['T_DAILY_MAX','T_DAILY_MIN','T_DAILY_MEAN','Daily Max 8-hour Ozone Concentration']].corr(method='pearson', numeric_only = True)  # We can visualize the correlation metrics by setting the style of the output. corr_matrix.style.background_gradient(cmap='Blues') Out[23]: T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN Daily Max 8-hour Ozone Concentration T_DAILY_MAX 1.000000 0.905145 0.978251 0.379406 T_DAILY_MIN 0.905145 1.000000 0.973623 0.206968 T_DAILY_MEAN 0.978251 0.973623 1.000000 0.304737 Daily Max 8-hour Ozone Concentration 0.379406 0.206968 0.304737 1.000000 In\u00a0[24]: Copied! <pre>import statsmodels.api as sm\n</pre> import statsmodels.api as sm In\u00a0[25]: Copied! <pre># Define the independent (X) and dependent (y) variables\nX = df_AQS['T_DAILY_MAX']  # Independent variable\ny = df_AQS['Daily Max 8-hour Ozone Concentration']*1000.  # Dependent variable\n</pre> # Define the independent (X) and dependent (y) variables X = df_AQS['T_DAILY_MAX']  # Independent variable y = df_AQS['Daily Max 8-hour Ozone Concentration']*1000.  # Dependent variable  In\u00a0[26]: Copied! <pre># Add a constant (intercept) to X\nX_with_const = sm.add_constant(X)\n\nmodel = sm.OLS(y, X_with_const).fit()\n</pre> # Add a constant (intercept) to X X_with_const = sm.add_constant(X)  model = sm.OLS(y, X_with_const).fit()  In\u00a0[27]: Copied! <pre>model.summary()\n</pre> model.summary() Out[27]: OLS Regression Results Dep. Variable: Daily Max 8-hour Ozone Concentration   R-squared:             0.144 Model: OLS   Adj. R-squared:        0.141 Method: Least Squares   F-statistic:           56.67 Date: Sun, 29 Sep 2024   Prob (F-statistic): 4.76e-13 Time: 15:19:23   Log-Likelihood:      -1226.9 No. Observations:    339   AIC:                   2458. Df Residuals:    337   BIC:                   2465. Df Model:      1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const    32.7306     0.903    36.251  0.000    30.955    34.507 T_DAILY_MAX     0.3396     0.045     7.528  0.000     0.251     0.428 Omnibus:  2.910   Durbin-Watson:         0.972 Prob(Omnibus):  0.233   Jarque-Bera (JB):      2.221 Skew:  0.007   Prob(JB):              0.329 Kurtosis:  2.604   Cond. No.               36.8 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[28]: Copied! <pre># Extract the intercept and slope\nintercept, slope = model.params\n\n# Print the slope and intercept\nprint(f\"Intercept: {intercept}\")\nprint(f\"Slope (Coefficient): {slope}\")\n</pre> # Extract the intercept and slope intercept, slope = model.params  # Print the slope and intercept print(f\"Intercept: {intercept}\") print(f\"Slope (Coefficient): {slope}\")  <pre>Intercept: 32.73055575275309\nSlope (Coefficient): 0.33960906163846516\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>If you have multiple independent variables, the process is similar. You just need to include more columns in your X.</p> In\u00a0[29]: Copied! <pre># Define the independent (X) and dependent (y) variables\nX = df_AQS[['T_DAILY_MAX','RH_DAILY_MAX']]  # Independent variables\ny = df_AQS['Daily Max 8-hour Ozone Concentration']*1000.  # Dependent variable\n</pre> # Define the independent (X) and dependent (y) variables X = df_AQS[['T_DAILY_MAX','RH_DAILY_MAX']]  # Independent variables y = df_AQS['Daily Max 8-hour Ozone Concentration']*1000.  # Dependent variable  In\u00a0[30]: Copied! <pre># Add a constant (intercept) to X\nX_with_const = sm.add_constant(X)\n\n# Fit the linear regression model\nmodel = sm.OLS(y, X_with_const).fit()\n\n# Get the regression results summary\nmodel.summary()\n</pre> # Add a constant (intercept) to X X_with_const = sm.add_constant(X)  # Fit the linear regression model model = sm.OLS(y, X_with_const).fit()  # Get the regression results summary model.summary() Out[30]: OLS Regression Results Dep. Variable: Daily Max 8-hour Ozone Concentration   R-squared:             0.205 Model: OLS   Adj. R-squared:        0.200 Method: Least Squares   F-statistic:           43.34 Date: Sun, 29 Sep 2024   Prob (F-statistic): 1.80e-17 Time: 15:19:53   Log-Likelihood:      -1214.4 No. Observations:    339   AIC:                   2435. Df Residuals:    336   BIC:                   2446. Df Model:      2 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const    59.2369     5.287    11.203  0.000    48.836    69.638 T_DAILY_MAX     0.4494     0.049     9.246  0.000     0.354     0.545 RH_DAILY_MAX    -0.3274     0.064    -5.083  0.000    -0.454    -0.201 Omnibus:  3.131   Durbin-Watson:         1.031 Prob(Omnibus):  0.209   Jarque-Bera (JB):      2.347 Skew:  0.017   Prob(JB):              0.309 Kurtosis:  2.594   Cond. No.               988. Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Week_6_Correlation_Regression_Analysis/#week-6-correlation-and-regressions-in-python","title":"Week 6: Correlation and Regressions in Python\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#1-linking-datasets-in-python","title":"1. Linking Datasets in Python\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#exercise-11-combine-the-two-dataframes-by-adding-daily-max-8-hour-ozone-concentration-from-df_aqs-to-df_weather","title":"Exercise 1.1: Combine the two dataframes by adding 'Daily Max 8-hour Ozone Concentration' from <code>df_AQS</code> to <code>df_weather</code>.\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#exercise-12-use-describe-to-get-the-basic-statistics-of-the-combined-dataset-df_aqs-and-df_weather-can-you-tell-whats-the-difference-between-these-two-dataframes","title":"Exercise 1.2: Use <code>describe</code> to get the basic statistics of the combined dataset df_AQS and df_weather. Can you tell what's the difference between these two dataframes?\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#2-correlation-analysis","title":"2. Correlation Analysis\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#21-scatter-plots","title":"2.1 Scatter Plots\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#22-pearson-correlation-coefficient","title":"2.2 Pearson Correlation Coefficient\u00b6","text":"<p>The Pearson correlation coefficient (often denoted as r) is a statistical measure that quantifies the strength and direction of the linear relationship between two continuous variables.</p>"},{"location":"Week_6_Correlation_Regression_Analysis/#exercise-2-using-the-above-formula-to-calculate-the-pearson-correlation-coefficient-r-between-daily-max-8-hour-ozone-concentration-and-t_daily_mean-in-df_aqs","title":"Exercise 2: Using the above formula to calculate the Pearson Correlation Coefficient r between 'Daily Max 8-hour Ozone Concentration' and 'T_DAILY_MEAN' in <code>df_AQS</code>\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#23-spearman-rank-correlation-coefficient","title":"2.3 Spearman Rank Correlation Coefficient\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#24-correlation-matrix","title":"2.4 Correlation Matrix\u00b6","text":"<p>Correlation matrix is a numerical representation (a matrix of the correlation coefficient (r)) showing the strength of the relationship among all the variables in your dataset.</p>"},{"location":"Week_6_Correlation_Regression_Analysis/#3-linear-regression","title":"3. Linear Regression\u00b6","text":"<p>There are multiple ways to conduct linear regression in Python. Here we use <code>statsmodels</code> library in Python, which provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.</p>"},{"location":"Week_6_Correlation_Regression_Analysis/#exercise-3-based-on-what-you-learned-from-the-video-explain-the-linear-regression-results-summary","title":"Exercise 3: Based on what you learned from the video, explain the linear regression results summary:\u00b6","text":"<p>A. What do the slope and intercept mean here? What are the their units?</p> <p>B. What does R-squared mean here?</p> <p>C. Using the linear relationship, predict the level of ozone at 30 \u02daC.</p> <p>D. What is degree of freedom of this linear model?</p>"},{"location":"Week_6_Correlation_Regression_Analysis/#4-multiple-linear-regression","title":"4. Multiple Linear Regression\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#exercise-4-based-on-what-you-learned-from-the-video-explain-the-multiple-linear-regression-results-summary","title":"Exercise 4: Based on what you learned from the video, explain the multiple linear regression results summary:\u00b6","text":"<p>A. What do the coefficients mean here? What are the their units?</p> <p>B. What is degree of freedom of this model?</p> <p>C. What is the difference between R-squared and Adj. R-squared?</p>"},{"location":"Week_6_Correlation_Regression_Analysis/#5-assignment-apply-correlation-and-regression-analysis-to-the-datasets-you-found-in-week-2-or-any-other-datasets-that-you-use","title":"5. Assignment: Apply correlation and regression analysis to the datasets you found in Week 2 (or any other datasets that you use).\u00b6","text":"<p>Note: If you are both careful and lucky (i.e. you are lucky enough to choose the right data set), you should be able to meld this weekly assignment into your final paper.</p>"},{"location":"Week_6_Correlation_Regression_Analysis/#51-in-one-sentence-explain-the-purpose-of-the-analysis-you-will-do","title":"5.1 In one sentence, explain the purpose of the analysis you will do.\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#52-correlation-analysis","title":"5.2 Correlation analysis\u00b6","text":""},{"location":"Week_6_Correlation_Regression_Analysis/#53-linear-regression-analysis","title":"5.3 Linear regression analysis\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/","title":"Week 7: Time series analysis","text":"<p>First, import python packages.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n</pre> import pandas as pd import numpy as np from matplotlib import pyplot as plt <p>Like the data we are working with in this exercise, many environmental datasets include timed records. The standard datetime library is the primary way of manipulating dates and times in Python, but there are additional third-party packages that provide additional support.</p> <p>A few worth exploring are dateutil, an extension of the datetime library useful for parsing timestamps, and pytz, which provides a smooth way of tackling time zones.</p> <p>Though we will not review datetime objects in depth here, it is useful to understand the basics of how to deal with datetime objects in Python as this is a main component of time series data.</p> <p>Here we will introduce a few pandas functions built on the datetime library to handle datetime objects.</p> In\u00a0[3]: Copied! <pre># Define a range of dates. By default, the frequency is daily. \npd.date_range('4/1/2017','4/30/2017')\n</pre> # Define a range of dates. By default, the frequency is daily.  pd.date_range('4/1/2017','4/30/2017') Out[3]: <pre>DatetimeIndex(['2017-04-01', '2017-04-02', '2017-04-03', '2017-04-04',\n               '2017-04-05', '2017-04-06', '2017-04-07', '2017-04-08',\n               '2017-04-09', '2017-04-10', '2017-04-11', '2017-04-12',\n               '2017-04-13', '2017-04-14', '2017-04-15', '2017-04-16',\n               '2017-04-17', '2017-04-18', '2017-04-19', '2017-04-20',\n               '2017-04-21', '2017-04-22', '2017-04-23', '2017-04-24',\n               '2017-04-25', '2017-04-26', '2017-04-27', '2017-04-28',\n               '2017-04-29', '2017-04-30'],\n              dtype='datetime64[ns]', freq='D')</pre> In\u00a0[4]: Copied! <pre># Specify start and end, monthly frequency\n\npd.date_range('4/1/2017','4/30/2018', freq = 'ME')\n</pre> # Specify start and end, monthly frequency  pd.date_range('4/1/2017','4/30/2018', freq = 'ME') Out[4]: <pre>DatetimeIndex(['2017-04-30', '2017-05-31', '2017-06-30', '2017-07-31',\n               '2017-08-31', '2017-09-30', '2017-10-31', '2017-11-30',\n               '2017-12-31', '2018-01-31', '2018-02-28', '2018-03-31',\n               '2018-04-30'],\n              dtype='datetime64[ns]', freq='ME')</pre> In\u00a0[5]: Copied! <pre># Specify start and end, 5min frequency\n# datetime64 is 64-bit integer, which represents an offset from 1970-01-01T00:00:00\n\npd.date_range('4/1/2017','4/30/2018', freq = '5min')\n</pre> # Specify start and end, 5min frequency # datetime64 is 64-bit integer, which represents an offset from 1970-01-01T00:00:00  pd.date_range('4/1/2017','4/30/2018', freq = '5min') Out[5]: <pre>DatetimeIndex(['2017-04-01 00:00:00', '2017-04-01 00:05:00',\n               '2017-04-01 00:10:00', '2017-04-01 00:15:00',\n               '2017-04-01 00:20:00', '2017-04-01 00:25:00',\n               '2017-04-01 00:30:00', '2017-04-01 00:35:00',\n               '2017-04-01 00:40:00', '2017-04-01 00:45:00',\n               ...\n               '2018-04-29 23:15:00', '2018-04-29 23:20:00',\n               '2018-04-29 23:25:00', '2018-04-29 23:30:00',\n               '2018-04-29 23:35:00', '2018-04-29 23:40:00',\n               '2018-04-29 23:45:00', '2018-04-29 23:50:00',\n               '2018-04-29 23:55:00', '2018-04-30 00:00:00'],\n              dtype='datetime64[ns]', length=113473, freq='5min')</pre> In\u00a0[6]: Copied! <pre>df = pd.read_csv('Millbrook_NY_daily_weather.csv')\n</pre> df = pd.read_csv('Millbrook_NY_daily_weather.csv') In\u00a0[7]: Copied! <pre>df.head()\n</pre> df.head() Out[7]: LST_DATE WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC ... SOIL_MOISTURE_10_DAILY SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 0 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 ... 0.233 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 1 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 ... 0.227 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 ... 0.223 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 3 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 ... 0.220 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 4 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 ... 0.213 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN <p>5 rows \u00d7 29 columns</p> In\u00a0[8]: Copied! <pre>df.columns\n</pre> df.columns Out[8]: <pre>Index(['LST_DATE', 'WBANNO', 'CRX_VN', 'LONGITUDE', 'LATITUDE', 'T_DAILY_MAX',\n       'T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_AVG', 'P_DAILY_CALC',\n       'SOLARAD_DAILY', 'SUR_TEMP_DAILY_TYPE', 'SUR_TEMP_DAILY_MAX',\n       'SUR_TEMP_DAILY_MIN', 'SUR_TEMP_DAILY_AVG', 'RH_DAILY_MAX',\n       'RH_DAILY_MIN', 'RH_DAILY_AVG', 'SOIL_MOISTURE_5_DAILY',\n       'SOIL_MOISTURE_10_DAILY', 'SOIL_MOISTURE_20_DAILY',\n       'SOIL_MOISTURE_50_DAILY', 'SOIL_MOISTURE_100_DAILY',\n       'SOIL_TEMP_5_DAILY', 'SOIL_TEMP_10_DAILY', 'SOIL_TEMP_20_DAILY',\n       'SOIL_TEMP_50_DAILY', 'SOIL_TEMP_100_DAILY', 'Unnamed: 28'],\n      dtype='object')</pre> <p>Which of the columns is likely to store the date and time information?</p> In\u00a0[9]: Copied! <pre>df['LST_DATE']\n</pre> df['LST_DATE'] Out[9]: <pre>0       2016-01-01\n1       2016-01-02\n2       2016-01-03\n3       2016-01-04\n4       2016-01-05\n           ...    \n2552    2022-12-27\n2553    2022-12-28\n2554    2022-12-29\n2555    2022-12-30\n2556    2022-12-31\nName: LST_DATE, Length: 2557, dtype: object</pre> <p>While the values certainly resemble datetime objects, they are stored in pandas as \"objects,\" which basically means that pandas doesn't recognize the data type \u2013 it doesn't know how to handle them.</p> <p>Using the pd.to_datetime() function, we can convert this column to datetime objects:</p> In\u00a0[10]: Copied! <pre>pd.to_datetime(df['LST_DATE'])\n</pre> pd.to_datetime(df['LST_DATE']) Out[10]: <pre>0      2016-01-01\n1      2016-01-02\n2      2016-01-03\n3      2016-01-04\n4      2016-01-05\n          ...    \n2552   2022-12-27\n2553   2022-12-28\n2554   2022-12-29\n2555   2022-12-30\n2556   2022-12-31\nName: LST_DATE, Length: 2557, dtype: datetime64[ns]</pre> In\u00a0[11]: Copied! <pre># Set the LST_DATE as datetime object. \ndf['LST_DATE'] = pd.to_datetime(df['LST_DATE'])\n</pre> # Set the LST_DATE as datetime object.  df['LST_DATE'] = pd.to_datetime(df['LST_DATE']) In\u00a0[24]: Copied! <pre># We can also set LST_DATE as datetime object so by setting the parse_dates when read in the csv data:\n\ndf = pd.read_csv('Millbrook_NY_daily_weather.csv', parse_dates = ['LST_DATE'])\n</pre> # We can also set LST_DATE as datetime object so by setting the parse_dates when read in the csv data:  df = pd.read_csv('Millbrook_NY_daily_weather.csv', parse_dates = ['LST_DATE']) In\u00a0[25]: Copied! <pre># Set the Date column as index:\n\ndf = df.set_index('LST_DATE')\n</pre> # Set the Date column as index:  df = df.set_index('LST_DATE') In\u00a0[26]: Copied! <pre># Now it's more intuitive to interpret the data:\n\ndf['T_DAILY_MEAN']\n</pre> # Now it's more intuitive to interpret the data:  df['T_DAILY_MEAN'] Out[26]: <pre>LST_DATE\n2016-01-01     1.5\n2016-01-02    -0.4\n2016-01-03     1.6\n2016-01-04    -6.9\n2016-01-05   -10.3\n              ... \n2022-12-27    -4.4\n2022-12-28     0.7\n2022-12-29     4.4\n2022-12-30    10.7\n2022-12-31     7.9\nName: T_DAILY_MEAN, Length: 2557, dtype: float64</pre> <p>After setting the date as the index, it's very simple to make time series plots in Pandas. You don't even need to set the x-axis label. Pandas will automatically decide the date frequency that can best show the date information!</p> In\u00a0[27]: Copied! <pre>df['T_DAILY_MEAN'].plot()\n</pre> df['T_DAILY_MEAN'].plot() Out[27]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>Since we have multiple years of data, Pandas will automatically choose to show the year only.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Now that we have a DatetimeIndex, we can access specific attributes of the datetime objects like the year, day, hour, etc. To do this, we add the desired time period using dot notation: df.index.attribute. For a full list of attributes, see the pd.DatetimeIndex documentation. For example:</p> In\u00a0[28]: Copied! <pre># Get the month of each record\ndf.index.month\n</pre> # Get the month of each record df.index.month Out[28]: <pre>Index([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n       ...\n       12, 12, 12, 12, 12, 12, 12, 12, 12, 12],\n      dtype='int32', name='LST_DATE', length=2557)</pre> In\u00a0[29]: Copied! <pre># Get the year of each record, and assign this to a new column\n\ndf['year'] = df.index.year\n</pre> # Get the year of each record, and assign this to a new column  df['year'] = df.index.year  In\u00a0[30]: Copied! <pre>df\n</pre> df Out[30]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 year LST_DATE 2016-01-01 64756 2.422 -73.74 41.79 3.4 -0.5 1.5 1.3 0.0 1.69 ... 0.204 0.155 0.147 4.2 4.4 5.1 6.0 7.6 NaN 2016 2016-01-02 64756 2.422 -73.74 41.79 2.9 -3.6 -0.4 -0.3 0.0 6.25 ... 0.199 0.152 0.144 2.8 3.1 4.2 5.7 7.4 NaN 2016 2016-01-03 64756 2.422 -73.74 41.79 5.1 -1.8 1.6 1.1 0.0 5.69 ... 0.196 0.151 0.141 2.6 2.8 3.8 5.2 7.2 NaN 2016 2016-01-04 64756 2.422 -73.74 41.79 0.5 -14.4 -6.9 -7.5 0.0 9.17 ... 0.194 0.148 0.139 1.7 2.1 3.4 4.9 6.9 NaN 2016 2016-01-05 64756 2.422 -73.74 41.79 -5.2 -15.5 -10.3 -11.7 0.0 9.34 ... 0.191 0.148 0.138 0.4 0.9 2.4 4.3 6.6 NaN 2016 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2022-12-27 64756 2.622 -73.74 41.79 -0.8 -8.0 -4.4 -3.8 0.0 4.00 ... NaN 0.164 0.157 -0.4 -0.2 0.5 2.2 4.0 NaN 2022 2022-12-28 64756 2.622 -73.74 41.79 7.4 -6.1 0.7 1.3 0.0 7.73 ... NaN 0.162 0.156 -0.4 -0.3 0.4 2.1 3.8 NaN 2022 2022-12-29 64756 2.622 -73.74 41.79 10.7 -1.8 4.4 5.0 0.0 6.66 ... NaN 0.159 0.155 -0.3 -0.3 0.3 1.9 3.7 NaN 2022 2022-12-30 64756 2.622 -73.74 41.79 16.6 4.9 10.7 10.3 0.0 5.39 ... NaN 0.159 0.154 -0.2 -0.2 0.3 1.8 3.6 NaN 2022 2022-12-31 64756 2.622 -73.74 41.79 13.2 2.7 7.9 10.2 5.0 1.25 ... NaN 0.160 0.153 -0.1 -0.2 0.3 1.8 3.4 NaN 2022 <p>2557 rows \u00d7 29 columns</p> In\u00a0[31]: Copied! <pre># Get the unique year values\ndf.index.year.unique()\n</pre> # Get the unique year values df.index.year.unique() Out[31]: <pre>Index([2016, 2017, 2018, 2019, 2020, 2021, 2022], dtype='int32', name='LST_DATE')</pre> In\u00a0[32]: Copied! <pre># Sometimes you may want to know the day of year:\n\ndf.index.dayofyear\n</pre> # Sometimes you may want to know the day of year:  df.index.dayofyear  Out[32]: <pre>Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       356, 357, 358, 359, 360, 361, 362, 363, 364, 365],\n      dtype='int32', name='LST_DATE', length=2557)</pre> In\u00a0[33]: Copied! <pre># You can also directly get the day of week, which is difficult to program on our own.\n\ndf.index.dayofweek\n</pre> # You can also directly get the day of week, which is difficult to program on our own.  df.index.dayofweek   Out[33]: <pre>Index([4, 5, 6, 0, 1, 2, 3, 4, 5, 6,\n       ...\n       3, 4, 5, 6, 0, 1, 2, 3, 4, 5],\n      dtype='int32', name='LST_DATE', length=2557)</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Below we resample the dataframe by taking the mean over each month.</p> In\u00a0[34]: Copied! <pre># First, we need to remove the columns that do not have numerical values.\ndf = df.drop(columns = 'SUR_TEMP_DAILY_TYPE')\n</pre> # First, we need to remove the columns that do not have numerical values. df = df.drop(columns = 'SUR_TEMP_DAILY_TYPE') In\u00a0[35]: Copied! <pre>df.resample('ME').mean().plot(y='T_DAILY_MEAN', marker='o')\n</pre> df.resample('ME').mean().plot(y='T_DAILY_MEAN', marker='o')  Out[35]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>Resampling can be applied to the entire dataframe.</p> In\u00a0[36]: Copied! <pre>df_mm = df.resample('ME').mean()\ndf_mm[['T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_MAX']].plot()\n</pre> df_mm = df.resample('ME').mean() df_mm[['T_DAILY_MIN', 'T_DAILY_MEAN', 'T_DAILY_MAX']].plot() Out[36]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>Just like with groupby, we can apply any aggregation function to our resample operation.</p> In\u00a0[37]: Copied! <pre># Resample by maximum values.\ndf.resample('ME').max().plot(y='T_DAILY_MAX', marker='o')\n</pre> # Resample by maximum values. df.resample('ME').max().plot(y='T_DAILY_MAX', marker='o')  Out[37]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>Rolling mean (or moving average) computes the average of data points in a sliding window over the series. It's typically used to smooth out short-term fluctuations and highlight longer-term trends.</p> In\u00a0[38]: Copied! <pre># Setting the window size to be 30. Center = True means the window is centered at a given index. For example, on 10/15/2022, it will return the mean from 10/01 to 10/30.\ndf.rolling(30, center=True).T_DAILY_MEAN.mean().plot()\n</pre> # Setting the window size to be 30. Center = True means the window is centered at a given index. For example, on 10/15/2022, it will return the mean from 10/01 to 10/30. df.rolling(30, center=True).T_DAILY_MEAN.mean().plot() Out[38]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>You may notice that there are some breaks in the time series. If there is missing value in the rolling window, pandas will return NaN for the rolling mean.</p> In\u00a0[39]: Copied! <pre># We find 12 observations with missing values for T_DAILY_MEAN. \ndf.loc[(df.T_DAILY_MEAN.isnull())]\n</pre> # We find 12 observations with missing values for T_DAILY_MEAN.  df.loc[(df.T_DAILY_MEAN.isnull())] Out[39]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 year LST_DATE 2017-10-04 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2017 2018-10-13 64756 2.622 -73.74 41.79 NaN NaN NaN NaN 4.6 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2018 2019-08-10 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2019-08-11 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2019-08-12 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2019-08-13 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2019-08-14 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2019-08-15 64756 -9.000 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2019-08-16 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2019-09-25 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2019 2021-01-04 64756 2.622 -73.74 41.79 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2021 2021-11-16 64756 2.622 -73.74 41.79 NaN NaN NaN NaN 0.0 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN 2021 <p>12 rows \u00d7 28 columns</p> <p>Instead of setting the number of observations, we could set the time period of each window. Each window will be a variable sized based on the observations included in the time-period.</p> In\u00a0[41]: Copied! <pre># Note that days with missing values will be skipped. \ndf.rolling('30D', center=True).T_DAILY_MEAN.mean().plot()\n</pre> # Note that days with missing values will be skipped.  df.rolling('30D', center=True).T_DAILY_MEAN.mean().plot()  Out[41]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Sometimes you may want to fill in the missing values. There are several ways to fill missing data in Pandas.</p> In\u00a0[43]: Copied! <pre># Let's first extract a subset of the data\ndf_sub = df.loc['2017-01-01':'2017-12-31']\n</pre> # Let's first extract a subset of the data df_sub = df.loc['2017-01-01':'2017-12-31'] In\u00a0[44]: Copied! <pre># Note there are data breaks in February.\ndf_sub['SOIL_MOISTURE_10_DAILY'].plot()\n</pre> # Note there are data breaks in February. df_sub['SOIL_MOISTURE_10_DAILY'].plot() Out[44]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[45]: Copied! <pre># Fill values forward using ffill function\n\ndf_sub.ffill()['SOIL_MOISTURE_10_DAILY'].plot(label = 'Forward')\n\n# Fill values backward using bfill function\n\ndf_sub.bfill()['SOIL_MOISTURE_10_DAILY'].plot(label = 'Backward')\n\n# Using interpolate function:\n\ndf_sub['SOIL_MOISTURE_10_DAILY'].interpolate('linear').plot(label = 'Linear Interploation')\n\nplt.legend()\n</pre> # Fill values forward using ffill function  df_sub.ffill()['SOIL_MOISTURE_10_DAILY'].plot(label = 'Forward')  # Fill values backward using bfill function  df_sub.bfill()['SOIL_MOISTURE_10_DAILY'].plot(label = 'Backward')  # Using interpolate function:  df_sub['SOIL_MOISTURE_10_DAILY'].interpolate('linear').plot(label = 'Linear Interploation')  plt.legend() Out[45]: <pre>&lt;matplotlib.legend.Legend at 0x3318f06d0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Recall the four major components of time series: seasonality, cycle, trend and variations. Many environmental datasets we deal with (e.g., temperature, precipitation, air quality) vary seasonally. A common way to analyze such data is to create a \"climatology,\" which contains the average values in each month or day of the year. We can do this easily with groupby. Recall that df.index is a pandas DateTimeIndex object.</p> In\u00a0[46]: Copied! <pre>monthly_climatology = df.groupby(df.index.month).mean(numeric_only=True)\nmonthly_climatology\n</pre> monthly_climatology = df.groupby(df.index.month).mean(numeric_only=True) monthly_climatology Out[46]: WBANNO CRX_VN LONGITUDE LATITUDE T_DAILY_MAX T_DAILY_MIN T_DAILY_MEAN T_DAILY_AVG P_DAILY_CALC SOLARAD_DAILY ... SOIL_MOISTURE_20_DAILY SOIL_MOISTURE_50_DAILY SOIL_MOISTURE_100_DAILY SOIL_TEMP_5_DAILY SOIL_TEMP_10_DAILY SOIL_TEMP_20_DAILY SOIL_TEMP_50_DAILY SOIL_TEMP_100_DAILY Unnamed: 28 year LST_DATE 1 64756.0 2.564857 -73.74 41.79 2.200000 -7.550463 -2.678241 -2.442130 2.217130 5.655139 ... 0.230433 0.164727 0.166809 0.393056 0.472685 0.963889 1.983796 3.371759 NaN 2019.000000 2 64756.0 2.564424 -73.74 41.79 4.866162 -5.901010 -0.519192 -0.220202 3.449495 8.359444 ... 0.219145 0.163520 0.165425 0.557071 0.518687 0.707071 1.256566 2.156061 NaN 2018.989899 3 64756.0 2.564857 -73.74 41.79 9.015668 -2.634101 3.184793 3.370046 2.426728 12.813917 ... 0.224130 0.168618 0.166180 3.385714 3.270507 3.157604 3.182488 3.337788 NaN 2019.000000 4 64756.0 2.564857 -73.74 41.79 14.930476 1.948095 8.438095 8.733810 3.217143 14.977381 ... 0.235376 0.165395 0.165433 9.685714 9.460476 8.950000 8.119524 7.199524 NaN 2019.000000 5 64756.0 2.564857 -73.74 41.79 20.996774 8.094009 14.548848 14.772811 3.182949 17.912673 ... 0.221042 0.156848 0.161083 16.721198 16.471889 15.606019 14.184793 12.509217 NaN 2019.000000 6 64756.0 2.564857 -73.74 41.79 25.874286 12.033810 18.952857 19.285714 2.290000 21.610095 ... 0.162114 0.136386 0.153190 22.399524 22.177619 21.112381 19.530476 17.661429 NaN 2019.000000 7 64756.0 2.564857 -73.74 41.79 29.040092 15.894470 22.465899 22.322120 3.880184 20.864101 ... 0.113350 0.118535 0.139206 25.527189 25.406912 24.316204 22.814286 21.129630 NaN 2019.000000 8 64756.0 2.297069 -73.74 41.79 28.139048 15.543810 21.838571 21.615714 3.969048 18.131429 ... 0.128214 0.122652 0.136627 24.912857 24.918095 24.244762 23.358571 22.286190 NaN 2019.000000 9 64756.0 2.564857 -73.74 41.79 23.720096 11.202871 17.460287 17.344019 3.948804 14.033301 ... 0.151890 0.127550 0.141841 20.640191 20.736364 20.594258 20.666986 20.554067 NaN 2019.000000 10 64756.0 2.590664 -73.74 41.79 17.773488 5.738140 11.753023 11.740465 3.637963 9.193674 ... 0.187633 0.141386 0.144750 14.663256 14.799070 15.074419 15.892558 16.645581 NaN 2019.000000 11 64756.0 2.593429 -73.74 41.79 10.377512 -1.277990 4.547368 4.720574 3.301905 6.567895 ... 0.237167 0.166742 0.161895 7.128708 7.302392 7.955288 9.385167 10.990909 NaN 2019.000000 12 64756.0 2.593429 -73.74 41.79 4.488018 -4.917972 -0.217512 0.055760 3.121198 4.639954 ... 0.242934 0.171530 0.170230 2.254839 2.385714 2.946083 4.288479 5.912442 NaN 2019.000000 <p>12 rows \u00d7 28 columns</p> <p>Each row in this new dataframe respresents the average values for the months (1=January, 2=February, etc.)</p> <p>We can apply more customized aggregations, as with any groupby operation. Below we keep the mean of the mean, max of the max, and min of the min for the temperature measurements.</p> In\u00a0[47]: Copied! <pre>monthly_T_climatology = df.groupby(df.index.month).aggregate({'T_DAILY_MEAN': 'mean',\n                                                              'T_DAILY_MAX': 'max',\n                                                              'T_DAILY_MIN': 'min'})\nmonthly_T_climatology\n</pre> monthly_T_climatology = df.groupby(df.index.month).aggregate({'T_DAILY_MEAN': 'mean',                                                               'T_DAILY_MAX': 'max',                                                               'T_DAILY_MIN': 'min'}) monthly_T_climatology Out[47]: T_DAILY_MEAN T_DAILY_MAX T_DAILY_MIN LST_DATE 1 -2.678241 19.8 -26.0 2 -0.519192 24.9 -24.7 3 3.184793 26.8 -17.4 4 8.438095 30.6 -11.3 5 14.548848 33.4 -3.1 6 18.952857 34.5 1.5 7 22.465899 36.2 8.2 8 21.838571 36.5 6.0 9 17.460287 32.7 -1.6 10 11.753023 29.9 -5.9 11 4.547368 24.4 -15.9 12 -0.217512 17.9 -21.8 In\u00a0[48]: Copied! <pre>monthly_T_climatology.plot(marker='o')\n</pre> monthly_T_climatology.plot(marker='o') Out[48]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>If we want to do it on a finer scale, we can group by day of year.</p> In\u00a0[49]: Copied! <pre>daily_T_climatology = df.groupby(df.index.dayofyear).aggregate({'T_DAILY_MEAN': 'mean',\n                                                            'T_DAILY_MAX': 'max',\n                                                            'T_DAILY_MIN': 'min'})\ndaily_T_climatology.plot(marker='.')\n</pre> daily_T_climatology = df.groupby(df.index.dayofyear).aggregate({'T_DAILY_MEAN': 'mean',                                                             'T_DAILY_MAX': 'max',                                                             'T_DAILY_MIN': 'min'}) daily_T_climatology.plot(marker='.') Out[49]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>A common mode of time series analysis is to remove the seasonality or cyclic patterns from a signal to focus only on the variations or anomaly values. This can be accomplished with transformation.</p> In\u00a0[50]: Copied! <pre>def standardize(x):\n    return (x - x.mean())/x.std()\n\nanomaly = df.groupby(df.index.month).transform(standardize)\n</pre> def standardize(x):     return (x - x.mean())/x.std()  anomaly = df.groupby(df.index.month).transform(standardize)  In\u00a0[51]: Copied! <pre>anomaly.plot(y='T_DAILY_MEAN')\n</pre> anomaly.plot(y='T_DAILY_MEAN') Out[51]: <pre>&lt;Axes: xlabel='LST_DATE'&gt;</pre> <p>We learned about linear regression from previous week. We can similarly apply the linear regression to time series data to fit a linear trend.</p> In\u00a0[52]: Copied! <pre>import statsmodels.api as sm\n</pre> import statsmodels.api as sm <p>To focus on the trend component, we could first resample the data to annual scale, so that the other components of the time series (seasonality and variations) do not affect the trends. Note here we just introduce the most basic trend analysis. Check out Venier et al. (2012) for some advanced trend analysis models used in Environmental Science research.</p> In\u00a0[54]: Copied! <pre>df_year = df.resample('YE').mean()\n</pre> df_year = df.resample('YE').mean() In\u00a0[55]: Copied! <pre>df_year.index\n</pre> df_year.index Out[55]: <pre>DatetimeIndex(['2016-12-31', '2017-12-31', '2018-12-31', '2019-12-31',\n               '2020-12-31', '2021-12-31', '2022-12-31'],\n              dtype='datetime64[ns]', name='LST_DATE', freq='YE-DEC')</pre> <p>statsmodels do not recognize datetime object, so we need to convert Datetime into a numerical variable like float.</p> In\u00a0[56]: Copied! <pre># Define the independent (X) and dependent (y) variables\nX = df_year.index.year   # Independent variable is converted to  year.\ny = df_year['SOIL_MOISTURE_5_DAILY']  # Dependent variable\n</pre> # Define the independent (X) and dependent (y) variables X = df_year.index.year   # Independent variable is converted to  year. y = df_year['SOIL_MOISTURE_5_DAILY']  # Dependent variable  In\u00a0[57]: Copied! <pre># Add a constant (intercept) to X\nX_with_const = sm.add_constant(X)\n\n# Note that the data have missing values. If we want to skip the missing values, we need to set missing to be 'drop'.\nmodel = sm.OLS(y, X_with_const, missing = 'drop').fit()\n</pre> # Add a constant (intercept) to X X_with_const = sm.add_constant(X)  # Note that the data have missing values. If we want to skip the missing values, we need to set missing to be 'drop'. model = sm.OLS(y, X_with_const, missing = 'drop').fit()  In\u00a0[58]: Copied! <pre>pred_y = model.predict(X_with_const)\n</pre> pred_y = model.predict(X_with_const) In\u00a0[59]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>/Users/xjin/miniforge3/envs/esa_env/lib/python3.9/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 7 samples were given.\n  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n</pre> Out[59]: OLS Regression Results Dep. Variable: SOIL_MOISTURE_5_DAILY   R-squared:             0.379 Model: OLS   Adj. R-squared:        0.255 Method: Least Squares   F-statistic:           3.056 Date: Sun, 06 Oct 2024   Prob (F-statistic):  0.141 Time: 15:32:06   Log-Likelihood:       16.947 No. Observations:      7   AIC:                  -29.89 Df Residuals:      5   BIC:                  -30.00 Df Model:      1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] const   -16.7584     9.704    -1.727  0.145   -41.703     8.186 x1     0.0084     0.005     1.748  0.141    -0.004     0.021 Omnibus:    nan   Durbin-Watson:         2.243 Prob(Omnibus):    nan   Jarque-Bera (JB):      0.522 Skew:  0.306   Prob(JB):              0.770 Kurtosis:  1.811   Cond. No.           2.04e+06 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 2.04e+06. This might indicate that there arestrong multicollinearity or other numerical problems.  In\u00a0[61]: Copied! <pre>plt.plot(df_year.index,pred_y)\nplt.plot(df_year.index,y)\n</pre> plt.plot(df_year.index,pred_y) plt.plot(df_year.index,y) Out[61]: <pre>[&lt;matplotlib.lines.Line2D at 0x3366977f0&gt;]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>If the dataset you used for correlation analysis happens to have a temporal dimension, this should be a good dataset to use. This way, you could continue melding this assignment into your final paper.</p> <p>If you have problems finding a good dataset, you could use the ozone measurements we used last week (Millbrook_NY_daily_ozone_2022.csv).</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"Week_7_Time_Series_Analysis/#week-7-time-series-analysis","title":"Week 7: Time series analysis\u00b6","text":"<p>Learning Goals:</p> <ol> <li>Understand the Datetime objects in Python.</li> <li>Analyze the four components of time series: seasonality, cycle, trend and variations.</li> </ol>"},{"location":"Week_7_Time_Series_Analysis/#1-working-with-datetime-objects","title":"1. Working with Datetime objects\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#11-setting-date-range","title":"1.1 Setting date range\u00b6","text":"<p>The pd.date_range() function allows you to build a DatetimeIndex with a fixed frequency. This can be done by specifying a start date and an end date as follows:</p>"},{"location":"Week_7_Time_Series_Analysis/#12-dealing-with-existing-timestamps","title":"1.2 Dealing with existing timestamps\u00b6","text":"<p>Ideally the tabular data we use already have date and time information. In Pandas, we could set the column type as datetime object.</p> <p>Here we're going to use daily weather data at Millbrook, NY site. We used this dataset last week.</p>"},{"location":"Week_7_Time_Series_Analysis/#13-plotting-time-series-in-pandas","title":"1.3 Plotting time series in Pandas\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#exercise-1-make-a-time-series-plot-of-daily-maximum-and-minmum-temperature-you-should-see-two-lines","title":"Exercise 1: Make a time series plot of daily maximum and minmum temperature. You should see two lines.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#14-access-datetime-attributes","title":"1.4 Access datetime attributes\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#exercise-2-create-a-column-in-df-that-represents-the-month-of-the-date","title":"Exercise 2: Create a column in <code>df</code> that represents the month of the date.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#2-temporal-resampling","title":"2. Temporal resampling\u00b6","text":"<p>Another common operation we apply to time series is to change the resolution of a dataset by resampling in time. Pandas exposes this through the resample function. The resample periods are specified using pandas offset index syntax.</p>"},{"location":"Week_7_Time_Series_Analysis/#21-resample-function","title":"2.1 Resample function\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#22-rolling-operations","title":"2.2 Rolling Operations\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#exercise-3-calculate-and-plot-10-day-rolling-mean-of-daily-max-temperature-t_daily_max","title":"Exercise 3: Calculate and plot 10-day rolling mean of daily max temperature <code>T_DAILY_MAX</code>.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#23-filling-missing-data","title":"2.3 Filling missing data\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#exercise-4-based-on-the-above-figure-explain-what-is-the-difference-among-ffill-bfill-and-interpolate","title":"Exercise 4: Based on the above figure, explain what is the difference among <code>ffill</code>, <code>bfill</code> and <code>interpolate</code>.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#3-analyze-the-seasonality-and-cyclic-patterns-of-time-series","title":"3. Analyze the seasonality and cyclic patterns of time series\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#exercise-5-calculate-and-plot-the-seasonal-cycle-of-solar-radiation-solarad_daily-at-monthly-scale","title":"Exercise 5: Calculate and plot the seasonal cycle of solar radiation <code>SOLARAD_DAILY</code> at monthly scale.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#4-analyze-variations-or-anomalies","title":"4. Analyze variations or anomalies\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#5-trend-analysis","title":"5. Trend Analysis\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#exercise-6-in-the-above-example-what-is-the-long-term-trend-of-soil_moisture-is-the-trend-statistically-significant-p-value-005","title":"Exercise 6: In the above example, what is the long term trend of SOIL_MOISTURE? Is the trend statistically significant (p value &lt; 0.05)?\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#6-assignment-conduct-time-series-analysis-for-a-dataset-youve-found","title":"6. Assignment: Conduct time series analysis for a dataset you've found.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#61-find-out-the-column-that-has-date-or-time-information-read-in-the-file-using-pandas-set-the-parse_dates-to-be-the-columns-of-the-date-and-time","title":"6.1 Find out the column that has date or time information. Read in the file using Pandas. Set the <code>parse_dates</code> to be the column(s) of the date and time.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#62-make-a-time-series-plot-of-one-of-the-measurements-columns","title":"6.2 Make a time series plot of one of the measurements columns.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#63-creat-two-new-columns-year-and-month-to-represent-the-year-and-month-the-data","title":"6.3 Creat two new columns <code>year</code> and <code>month</code> to represent the year and month the data.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#64-calculate-and-plot-the-monthly-climatology-of-the-measurements","title":"6.4 Calculate and plot the monthly climatology of the measurements.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#65-plot-standardized-measurements-with-seasonal-cycle-removed","title":"6.5 Plot standardized measurements with seasonal cycle removed.\u00b6","text":""},{"location":"Week_7_Time_Series_Analysis/#66-calculate-the-long-term-trend-of-the-data-report-the-trend-and-statistical-significance-p-value-of-the-trend","title":"6.6 Calculate the long-term trend of the data. Report the trend and statistical significance (p value) of the trend.\u00b6","text":""}]}